{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duomenų valymas (nesutvarkytas duomenų tvarkymas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year    state state_po  state_fips  state_cen  state_ic    office  \\\n",
      "0  1976  ALABAMA       AL           1         63        41  US HOUSE   \n",
      "1  1976  ALABAMA       AL           1         63        41  US HOUSE   \n",
      "2  1976  ALABAMA       AL           1         63        41  US HOUSE   \n",
      "3  1976  ALABAMA       AL           1         63        41  US HOUSE   \n",
      "4  1976  ALABAMA       AL           1         63        41  US HOUSE   \n",
      "\n",
      "   district stage runoff  special                   candidate       party  \\\n",
      "0         1   GEN  False    False              BILL DAVENPORT    DEMOCRAT   \n",
      "1         1   GEN  False    False                JACK EDWARDS  REPUBLICAN   \n",
      "2         1   GEN  False    False                     WRITEIN         NaN   \n",
      "3         2   GEN  False    False             J CAROLE KEAHEY    DEMOCRAT   \n",
      "4         2   GEN  False    False  WILLIAM L \"BILL\" DICKINSON  REPUBLICAN   \n",
      "\n",
      "   writein   mode  candidatevotes  totalvotes  unofficial   version  \\\n",
      "0    False  TOTAL           58906      157170       False  20230706   \n",
      "1    False  TOTAL           98257      157170       False  20230706   \n",
      "2     True  TOTAL               7      157170       False  20230706   \n",
      "3    False  TOTAL           66288      156362       False  20230706   \n",
      "4    False  TOTAL           90069      156362       False  20230706   \n",
      "\n",
      "   fusion_ticket  \n",
      "0          False  \n",
      "1          False  \n",
      "2          False  \n",
      "3          False  \n",
      "4          False  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32452 entries, 0 to 32451\n",
      "Data columns (total 20 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   year            32452 non-null  int64 \n",
      " 1   state           32452 non-null  object\n",
      " 2   state_po        32452 non-null  object\n",
      " 3   state_fips      32452 non-null  int64 \n",
      " 4   state_cen       32452 non-null  int64 \n",
      " 5   state_ic        32452 non-null  int64 \n",
      " 6   office          32452 non-null  object\n",
      " 7   district        32452 non-null  int64 \n",
      " 8   stage           32452 non-null  object\n",
      " 9   runoff          23796 non-null  object\n",
      " 10  special         32452 non-null  bool  \n",
      " 11  candidate       32452 non-null  object\n",
      " 12  party           28594 non-null  object\n",
      " 13  writein         32452 non-null  bool  \n",
      " 14  mode            32452 non-null  object\n",
      " 15  candidatevotes  32452 non-null  int64 \n",
      " 16  totalvotes      32452 non-null  int64 \n",
      " 17  unofficial      32452 non-null  bool  \n",
      " 18  version         32452 non-null  int64 \n",
      " 19  fusion_ticket   32452 non-null  bool  \n",
      "dtypes: bool(4), int64(8), object(8)\n",
      "memory usage: 4.1+ MB\n",
      "None\n",
      "year                 0\n",
      "state                0\n",
      "state_po             0\n",
      "state_fips           0\n",
      "state_cen            0\n",
      "state_ic             0\n",
      "office               0\n",
      "district             0\n",
      "stage                0\n",
      "runoff            8656\n",
      "special              0\n",
      "candidate            0\n",
      "party             3858\n",
      "writein              0\n",
      "mode                 0\n",
      "candidatevotes       0\n",
      "totalvotes           0\n",
      "unofficial           0\n",
      "version              0\n",
      "fusion_ticket        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Nurodykite failo kelią su Atstovų Rūmų rinkimų duomenimis\n",
    "failo_kelias = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\house.csv\"\n",
    "\n",
    "# Užkrauname duomenis\n",
    "house_data = pd.read_csv(failo_kelias)\n",
    "\n",
    "# Peržiūrime pirmas kelias eilutes\n",
    "print(house_data.head())\n",
    "\n",
    "# Patikriname stulpelius ir jų tipą\n",
    "print(house_data.info())\n",
    "\n",
    "# Patikriname, ar yra trūkstamų reikšmių\n",
    "print(house_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aprasomoji statistika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       year         state  district                   candidate       party  \\\n",
      "1      1976       ALABAMA         1                JACK EDWARDS  REPUBLICAN   \n",
      "4      1976       ALABAMA         2  WILLIAM L \"BILL\" DICKINSON  REPUBLICAN   \n",
      "6      1976       ALABAMA         3                BILL NICHOLS    DEMOCRAT   \n",
      "10     1976       ALABAMA         4                  TOM BEVILL    DEMOCRAT   \n",
      "12     1976       ALABAMA         5             RONNIE G FLIPPO    DEMOCRAT   \n",
      "...     ...           ...       ...                         ...         ...   \n",
      "32428  2022      DELAWARE         0        LISA BLUNT ROCHESTER    DEMOCRAT   \n",
      "32431  2022  NORTH DAKOTA         0             KELLY ARMSTRONG  REPUBLICAN   \n",
      "32434  2022  SOUTH DAKOTA         0               DUSTY JOHNSON  REPUBLICAN   \n",
      "32437  2022       VERMONT         0                BECCA BALINT    DEMOCRAT   \n",
      "32445  2022       WYOMING         0             HARRIET HAGEMAN  REPUBLICAN   \n",
      "\n",
      "       candidatevotes  \n",
      "1               98257  \n",
      "4               90069  \n",
      "6              106935  \n",
      "10             141490  \n",
      "12             113553  \n",
      "...               ...  \n",
      "32428          178416  \n",
      "32431          148399  \n",
      "32434          253821  \n",
      "32437          176494  \n",
      "32445          132206  \n",
      "\n",
      "[10441 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paulius\\AppData\\Local\\Temp\\ipykernel_28232\\3731061359.py:2: FutureWarning: The provided callable <built-in function max> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  house_data['winner'] = house_data.groupby(['year', 'state', 'district'])['candidatevotes'].transform(max) == house_data['candidatevotes']\n"
     ]
    }
   ],
   "source": [
    "# Sukuriame stulpelį, kuris identifikuoja laimėtoją kiekvienoje apygardoje\n",
    "house_data[\"winner\"] = house_data.groupby([\"year\", \"state\", \"district\"])[\"candidatevotes\"].transform(max) == house_data[\"candidatevotes\"]\n",
    "house_data[\"winner\"] = house_data[\"winner\"].astype(int)\n",
    "\n",
    "# Filtruojame tik laimėjusius kandidatus\n",
    "winners = house_data[house_data[\"winner\"] == 1]\n",
    "print(winners[[\"year\", \"state\", \"district\", \"candidate\", \"party\", \"candidatevotes\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year    state state_po  state_fips  state_cen  state_ic    office  \\\n",
      "0  1976  ALABAMA       AL           1         63        41  US HOUSE   \n",
      "1  1976  ALABAMA       AL           1         63        41  US HOUSE   \n",
      "2  1976  ALABAMA       AL           1         63        41  US HOUSE   \n",
      "3  1976  ALABAMA       AL           1         63        41  US HOUSE   \n",
      "4  1976  ALABAMA       AL           1         63        41  US HOUSE   \n",
      "\n",
      "   district stage runoff  special                   candidate       party  \\\n",
      "0         1   GEN  False    False              BILL DAVENPORT    DEMOCRAT   \n",
      "1         1   GEN  False    False                JACK EDWARDS  REPUBLICAN   \n",
      "2         1   GEN  False    False                     WRITEIN         NaN   \n",
      "3         2   GEN  False    False             J CAROLE KEAHEY    DEMOCRAT   \n",
      "4         2   GEN  False    False  WILLIAM L \"BILL\" DICKINSON  REPUBLICAN   \n",
      "\n",
      "   writein   mode  candidatevotes  totalvotes  unofficial   version  \\\n",
      "0    False  TOTAL           58906      157170       False  20230706   \n",
      "1    False  TOTAL           98257      157170       False  20230706   \n",
      "2     True  TOTAL               7      157170       False  20230706   \n",
      "3    False  TOTAL           66288      156362       False  20230706   \n",
      "4    False  TOTAL           90069      156362       False  20230706   \n",
      "\n",
      "   fusion_ticket  \n",
      "0          False  \n",
      "1          False  \n",
      "2          False  \n",
      "3          False  \n",
      "4          False  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32452 entries, 0 to 32451\n",
      "Data columns (total 20 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   year            32452 non-null  int64 \n",
      " 1   state           32452 non-null  object\n",
      " 2   state_po        32452 non-null  object\n",
      " 3   state_fips      32452 non-null  int64 \n",
      " 4   state_cen       32452 non-null  int64 \n",
      " 5   state_ic        32452 non-null  int64 \n",
      " 6   office          32452 non-null  object\n",
      " 7   district        32452 non-null  int64 \n",
      " 8   stage           32452 non-null  object\n",
      " 9   runoff          23796 non-null  object\n",
      " 10  special         32452 non-null  bool  \n",
      " 11  candidate       32452 non-null  object\n",
      " 12  party           28594 non-null  object\n",
      " 13  writein         32452 non-null  bool  \n",
      " 14  mode            32452 non-null  object\n",
      " 15  candidatevotes  32452 non-null  int64 \n",
      " 16  totalvotes      32452 non-null  int64 \n",
      " 17  unofficial      32452 non-null  bool  \n",
      " 18  version         32452 non-null  int64 \n",
      " 19  fusion_ticket   32452 non-null  bool  \n",
      "dtypes: bool(4), int64(8), object(8)\n",
      "memory usage: 4.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Nurodykite failo kelią su Atstovų Rūmų rinkimų duomenimis\n",
    "failo_kelias = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\house.csv\"\n",
    "\n",
    "# Užkrauname duomenis\n",
    "house_data = pd.read_csv(failo_kelias)\n",
    "\n",
    "# Peržiūrime pirmas kelias eilutes\n",
    "print(house_data.head())\n",
    "\n",
    "# Patikriname stulpelius ir jų tipą\n",
    "print(house_data.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year    state state_po  state_fips  state_cen  state_ic    office  \\\n",
      "0  1976  ALABAMA       AL           1         63        41  US HOUSE   \n",
      "1  1976  ALABAMA       AL           1         63        41  US HOUSE   \n",
      "2  1976  ALABAMA       AL           1         63        41  US HOUSE   \n",
      "3  1976  ALABAMA       AL           1         63        41  US HOUSE   \n",
      "4  1976  ALABAMA       AL           1         63        41  US HOUSE   \n",
      "\n",
      "   district stage runoff  ...                   candidate       party writein  \\\n",
      "0         1   GEN  False  ...              BILL DAVENPORT    DEMOCRAT   False   \n",
      "1         1   GEN  False  ...                JACK EDWARDS  REPUBLICAN   False   \n",
      "2         1   GEN  False  ...                     WRITEIN         NaN    True   \n",
      "3         2   GEN  False  ...             J CAROLE KEAHEY    DEMOCRAT   False   \n",
      "4         2   GEN  False  ...  WILLIAM L \"BILL\" DICKINSON  REPUBLICAN   False   \n",
      "\n",
      "    mode candidatevotes  totalvotes  unofficial   version  fusion_ticket  \\\n",
      "0  TOTAL          58906      157170       False  20230706          False   \n",
      "1  TOTAL          98257      157170       False  20230706          False   \n",
      "2  TOTAL              7      157170       False  20230706          False   \n",
      "3  TOTAL          66288      156362       False  20230706          False   \n",
      "4  TOTAL          90069      156362       False  20230706          False   \n",
      "\n",
      "   winner  \n",
      "0       0  \n",
      "1       1  \n",
      "2       0  \n",
      "3       0  \n",
      "4       1  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paulius\\AppData\\Local\\Temp\\ipykernel_28232\\1055009956.py:2: FutureWarning: The provided callable <built-in function max> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  house_data['winner'] = house_data.groupby(['year', 'state', 'district'])['candidatevotes'].transform(max) == house_data['candidatevotes']\n"
     ]
    }
   ],
   "source": [
    "# Sukuriame \"winner\" stulpelį\n",
    "house_data[\"winner\"] = house_data.groupby([\"year\", \"state\", \"district\"])[\"candidatevotes\"].transform(max) == house_data[\"candidatevotes\"]\n",
    "house_data[\"winner\"] = house_data[\"winner\"].astype(int)\n",
    "\n",
    "# Patikrinkite, ar stulpelis egzistuoja\n",
    "print(house_data.head())  # Įsitikinkite, kad yra \"winner\" stulpelis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Susiinstaliuoju pytrends, norėdamas prideti i duomenis gooogle labiausiia ieskomu kandidatu skaicius, hipoteze tokia kad populiaresnis kandidatas turi daugiau sancu buti isrinktas, nei maziau populiarus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytrends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paieškų kiekis kandidatui Donald Trump Alabamoje: 1340\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pytrends.request import TrendReq\n",
    "\n",
    "# Sukuriame „pytrends“ užklausos objektą\n",
    "pytrends = TrendReq(hl=\"en-US\", tz=360)\n",
    "\n",
    "\n",
    "# Funkcija, skirta paieškos duomenims surinkti kandidatams per laiką\n",
    "def gauti_paieskos_duomenis(kandidatas, geo, start_date, end_date):\n",
    "    pytrends.build_payload([kandidatas], timeframe=f\"{start_date} {end_date}\", geo=geo)\n",
    "    data = pytrends.interest_over_time()\n",
    "    if not data.empty:\n",
    "        return data[kandidatas].sum()  # Grąžiname bendrą paieškų skaičių\n",
    "    return 0  # Jei duomenų nėra, grąžiname 0\n",
    "\n",
    "\n",
    "# Pavyzdys: Rinkimų kandidato paieškų kiekis Alabamos valstijoje nuo 2020-09-01 iki 2020-11-03 (rinkimų diena)\n",
    "kandidatas = \"Donald Trump\"\n",
    "state_code = \"US-AL\"  # Alabamos kodas\n",
    "start_date = \"2020-09-01\"\n",
    "end_date = \"2020-11-03\"\n",
    "\n",
    "paieskos_kiekis = gauti_paieskos_duomenis(kandidatas, state_code, start_date, end_date)\n",
    "print(f\"Paieškų kiekis kandidatui {kandidatas} Alabamoje: {paieskos_kiekis}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from pytrends.request import TrendReq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nurodykite failo kelią su Atstovų Rūmų rinkimų duomenimis\n",
    "failo_kelias = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\house.csv\"\n",
    "house_data = pd.read_csv(failo_kelias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sukuriame „pytrends“ užklausos objektą\n",
    "pytrends = TrendReq(hl=\"en-US\", tz=360)\n",
    "\n",
    "\n",
    "# Funkcija, skirta paieškos duomenims gauti kandidatams per laiką ir valstiją\n",
    "def gauti_paieskos_duomenis(kandidatas, geo, start_date, end_date):\n",
    "    pytrends.build_payload([kandidatas], timeframe=f\"{start_date} {end_date}\", geo=geo)\n",
    "    data = pytrends.interest_over_time()\n",
    "    if not data.empty:\n",
    "        return data[kandidatas].sum()  # Grąžiname bendrą paieškų skaičių per nurodytą laikotarpį\n",
    "    return 0  # Jei duomenų nėra, grąžiname 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pridėkime stulpelį „populiarumas_google“ kiekvienam kandidatui\n",
    "house_data[\"populiarumas_google\"] = house_data.apply(\n",
    "    lambda row: gauti_paieskos_duomenis(\n",
    "        kandidatas=row[\"candidate\"],\n",
    "        geo=f\"US-{row[\"state_po\"]}\",  # Valstijos kodas\n",
    "        start_date=\"2020-09-01\",  # Pradžios data (pavyzdys: 2 mėn. iki rinkimų)\n",
    "        end_date=\"2020-11-03\"  # Pabaigos data (pavyzdys: rinkimų diena)\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Patikrinkime pirmas eilutes\n",
    "print(house_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytrends.request import TrendReq\n",
    "\n",
    "# Sukuriame „pytrends“ užklausos objektą\n",
    "pytrends = TrendReq(hl=\"en-US\", tz=360)\n",
    "\n",
    "\n",
    "# Funkcija, skirta gauti paieškos duomenis kandidatams per laiką ir valstiją\n",
    "def gauti_paieskos_duomenis(kandidatas, geo, start_date, end_date):\n",
    "    try:\n",
    "        pytrends.build_payload([kandidatas], timeframe=f\"{start_date} {end_date}\", geo=geo)\n",
    "        data = pytrends.interest_over_time()\n",
    "        if not data.empty:\n",
    "            return data[kandidatas].sum()  # Grąžiname bendrą paieškų skaičių per nurodytą laikotarpį\n",
    "        return 0  # Jei duomenų nėra, grąžiname 0\n",
    "    except Exception as e:\n",
    "        print(f\"Klaida gauti duomenis kandidatui {kandidatas} ({geo}): {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Nurodykite savo CSV failo kelią\n",
    "failo_kelias = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\house.csv\"\n",
    "house_data = pd.read_csv(failo_kelias)\n",
    "\n",
    "# Pridėkite paieškų duomenis su pauze tarp užklausų\n",
    "house_data[\"populiarumas_google\"] = None  # Sukuriame naują stulpelį\n",
    "\n",
    "for i, row in house_data.iterrows():\n",
    "    kandidatas = row[\"candidate\"]\n",
    "    geo = f\"US-{row[\"state_po\"]}\"\n",
    "    paieskos_kiekis = gauti_paieskos_duomenis(kandidatas, geo, \"2020-09-01\", \"2020-11-03\")\n",
    "    house_data.at[i, \"populiarumas_google\"] = paieskos_kiekis\n",
    "\n",
    "    # Pauzė tarp užklausų (pvz., 5 sekundės)\n",
    "    time.sleep(5)\n",
    "\n",
    "# Išsaugome atnaujintą failą\n",
    "naujas_failo_kelias = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\house_atnaujintas.csv\"\n",
    "house_data.to_csv(naujas_failo_kelias, index=False)\n",
    "\n",
    "print(\"Duomenys išsaugoti su pridėtu \"populiarumas_google\" stulpeliu!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klaida gauti duomenis kandidatui BILL DAVENPORT (US-AL, 1976): The request failed: Google returned a response with code 429\n",
      "Klaida gauti duomenis kandidatui JACK EDWARDS (US-AL, 1976): The request failed: Google returned a response with code 429\n",
      "Klaida gauti duomenis kandidatui WRITEIN (US-AL, 1976): The request failed: Google returned a response with code 429\n",
      "Klaida gauti duomenis kandidatui J CAROLE KEAHEY (US-AL, 1976): The request failed: Google returned a response with code 429\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m geo \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUS-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_po\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m year \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 34\u001b[0m paieskos_kiekis \u001b[38;5;241m=\u001b[39m \u001b[43mgauti_paieskos_duomenis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkandidatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m house_data\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpopuliarumas_google\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m paieskos_kiekis\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Pauzė tarp užklausų (pvz., 5 sekundės)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m, in \u001b[0;36mgauti_paieskos_duomenis\u001b[1;34m(kandidatas, geo, year)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     pytrends\u001b[38;5;241m.\u001b[39mbuild_payload([kandidatas], timeframe\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, geo\u001b[38;5;241m=\u001b[39mgeo)\n\u001b[1;32m---> 14\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpytrends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterest_over_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data[kandidatas]\u001b[38;5;241m.\u001b[39msum()  \u001b[38;5;66;03m# Grąžiname bendrą paieškų skaičių per nurodytą laikotarpį\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytrends\\request.py:232\u001b[0m, in \u001b[0;36mTrendReq.interest_over_time\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m over_time_payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;66;03m# convert to string as requests will mangle\u001b[39;00m\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreq\u001b[39m\u001b[38;5;124m'\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterest_over_time_widget[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterest_over_time_widget[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtz\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtz\n\u001b[0;32m    229\u001b[0m }\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# make the request and parse the returned json\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m req_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrendReq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINTEREST_OVER_TIME_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrendReq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET_METHOD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrim_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mover_time_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(req_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimelineData\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (df\u001b[38;5;241m.\u001b[39mempty):\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytrends\\request.py:140\u001b[0m, in \u001b[0;36mTrendReq._get_data\u001b[1;34m(self, url, method, trim_chars, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m     response \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mpost(url, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    137\u001b[0m                       cookies\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    138\u001b[0m                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequests_args)  \u001b[38;5;66;03m# DO NOT USE retries or backoff_factor here\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequests_args\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# DO NOT USE retries or backoff_factor here\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# check if the response contains json and throw an exception otherwise\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Google mostly sends 'application/json' in the Content-Type header,\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# but occasionally it sends 'application/javascript\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# and sometimes even 'text/javascript\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \\\n\u001b[0;32m    147\u001b[0m         response\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/javascript\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# some responses start with garbage characters, like \")]}',\"\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# these have to be cleaned before being passed to the json parser\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:693\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    692\u001b[0m     sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 693\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    694\u001b[0m     server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[0;32m    695\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[0;32m     75\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from pytrends.request import TrendReq\n",
    "\n",
    "# Sukuriame „pytrends“ užklausos objektą\n",
    "pytrends = TrendReq(hl=\"en-US\", tz=360)\n",
    "\n",
    "\n",
    "# Funkcija gauti paieškos duomenis kandidatams per laiką ir valstiją\n",
    "def gauti_paieskos_duomenis(kandidatas, geo, year):\n",
    "    start_date = f\"{year}-09-01\"\n",
    "    end_date = f\"{year}-10-31\"\n",
    "    try:\n",
    "        pytrends.build_payload([kandidatas], timeframe=f\"{start_date} {end_date}\", geo=geo)\n",
    "        data = pytrends.interest_over_time()\n",
    "        if not data.empty:\n",
    "            return data[kandidatas].sum()  # Grąžiname bendrą paieškų skaičių per nurodytą laikotarpį\n",
    "        return 0  # Jei duomenų nėra, grąžiname 0\n",
    "    except Exception as e:\n",
    "        print(f\"Klaida gauti duomenis kandidatui {kandidatas} ({geo}, {year}): {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Nurodykite savo CSV failo kelią\n",
    "failo_kelias = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\house.csv\"\n",
    "house_data = pd.read_csv(failo_kelias)\n",
    "\n",
    "# Pridėkite paieškų duomenis su pauze tarp užklausų\n",
    "house_data[\"populiarumas_google\"] = None  # Sukuriame naują stulpelį\n",
    "\n",
    "for i, row in house_data.iterrows():\n",
    "    kandidatas = row[\"candidate\"]\n",
    "    geo = f\"US-{row[\"state_po\"]}\"\n",
    "    year = row[\"year\"]\n",
    "    paieskos_kiekis = gauti_paieskos_duomenis(kandidatas, geo, year)\n",
    "    house_data.at[i, \"populiarumas_google\"] = paieskos_kiekis\n",
    "\n",
    "    # Pauzė tarp užklausų (pvz., 5 sekundės)\n",
    "    time.sleep(5)\n",
    "\n",
    "# Išsaugome atnaujintą failą\n",
    "naujas_failo_kelias = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\house_atnaujintas.csv\"\n",
    "house_data.to_csv(naujas_failo_kelias, index=False)\n",
    "\n",
    "print(\"Duomenys išsaugoti su pridėtu \"populiarumas_google\" stulpeliu!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google trends turi duomenis tik nuo 2004 metu, todėl pakeičiu koda kad tikrintu tik nuo tų metų"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m     house_data\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpopuliarumas_google\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m paieskos_kiekis\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Pauzė tarp užklausų (pvz., 5 sekundės)\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Išsaugome atnaujintą failą\u001b[39;00m\n\u001b[0;32m     45\u001b[0m naujas_failo_kelias \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mPaulius\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDuomenu mokslas\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mprojektas_US_rinkimai\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mhouse_atnaujintas.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from pytrends.request import TrendReq\n",
    "\n",
    "# Sukuriame „pytrends“ užklausos objektą\n",
    "pytrends = TrendReq(hl=\"en-US\", tz=360)\n",
    "\n",
    "\n",
    "# Funkcija gauti paieškos duomenis kandidatams per laiką ir valstiją\n",
    "def gauti_paieskos_duomenis(kandidatas, geo, year):\n",
    "    if year < 2004:\n",
    "        # Jei metai yra prieš 2004, grąžiname None arba 0 (pagal poreikį)\n",
    "        return None  # Arba galite naudoti 0, pvz., `return 0`\n",
    "\n",
    "    start_date = f\"{year}-09-01\"\n",
    "    end_date = f\"{year}-10-31\"\n",
    "    try:\n",
    "        pytrends.build_payload([kandidatas], timeframe=f\"{start_date} {end_date}\", geo=geo)\n",
    "        data = pytrends.interest_over_time()\n",
    "        if not data.empty:\n",
    "            return data[kandidatas].sum()  # Grąžiname bendrą paieškų skaičių per nurodytą laikotarpį\n",
    "        return 0  # Jei duomenų nėra, grąžiname 0\n",
    "    except Exception as e:\n",
    "        print(f\"Klaida gauti duomenis kandidatui {kandidatas} ({geo}, {year}): {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Nurodykite savo CSV failo kelią\n",
    "failo_kelias = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\house.csv\"\n",
    "house_data = pd.read_csv(failo_kelias)\n",
    "\n",
    "# Pridėkite paieškų duomenis su pauze tarp užklausų\n",
    "house_data[\"populiarumas_google\"] = None  # Sukuriame naują stulpelį\n",
    "\n",
    "for i, row in house_data.iterrows():\n",
    "    kandidatas = row[\"candidate\"]\n",
    "    geo = f\"US-{row[\"state_po\"]}\"\n",
    "    year = row[\"year\"]\n",
    "    paieskos_kiekis = gauti_paieskos_duomenis(kandidatas, geo, year)\n",
    "    house_data.at[i, \"populiarumas_google\"] = paieskos_kiekis\n",
    "\n",
    "    # Pauzė tarp užklausų (pvz., 5 sekundės)\n",
    "    time.sleep(5)\n",
    "\n",
    "# Išsaugome atnaujintą failą\n",
    "naujas_failo_kelias = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\house_atnaujintas.csv\"\n",
    "house_data.to_csv(naujas_failo_kelias, index=False)\n",
    "\n",
    "print(\"Duomenys išsaugoti su pridėtu \"populiarumas_google\" stulpeliu!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Išsaugome atnaujintą failą\n",
    "naujas_failo_kelias = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\house_atnaujintas.csv\"\n",
    "house_data.to_csv(naujas_failo_kelias, index=False)\n",
    "\n",
    "print(\"Duomenys išsaugoti su pridėtu \"populiarumas_google\" stulpeliu!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from checkpoint.\n",
      "Error retrieving data for candidate ROBERT J HARMS (FLORIDA, 2006): The request failed: Google returned a response with code 429\n",
      "Processed candidate: ROBERT J HARMS (FLORIDA, 2006) - Popularity: None\n",
      "Error retrieving data for candidate WRITEIN (FLORIDA, 2006): The request failed: Google returned a response with code 429\n",
      "Processed candidate: WRITEIN (FLORIDA, 2006) - Popularity: None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 76\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotna(house_data\u001b[38;5;241m.\u001b[39mloc[(house_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcandidate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m kandidatas) \u001b[38;5;241m&\u001b[39m\n\u001b[0;32m     72\u001b[0m                            (house_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m state) \u001b[38;5;241m&\u001b[39m\n\u001b[0;32m     73\u001b[0m                            (house_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m year), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpopuliarumas_google\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m paieskos_kiekis \u001b[38;5;241m=\u001b[39m \u001b[43mgauti_paieskos_duomenis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkandidatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Save data by candidate, state, and year\u001b[39;00m\n\u001b[0;32m     79\u001b[0m house_data\u001b[38;5;241m.\u001b[39mloc[(house_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcandidate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m kandidatas) \u001b[38;5;241m&\u001b[39m\n\u001b[0;32m     80\u001b[0m                (house_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m state) \u001b[38;5;241m&\u001b[39m\n\u001b[0;32m     81\u001b[0m                (house_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m year), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpopuliarumas_google\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m paieskos_kiekis\n",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m, in \u001b[0;36mgauti_paieskos_duomenis\u001b[1;34m(kandidatas, state, year)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Adding state to the keyword for better accuracy\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     search_term \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkandidatas\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 25\u001b[0m     \u001b[43mpytrends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msearch_term\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstart_date\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mend_date\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     data \u001b[38;5;241m=\u001b[39m pytrends\u001b[38;5;241m.\u001b[39minterest_over_time()\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data\u001b[38;5;241m.\u001b[39mempty:\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytrends\\request.py:189\u001b[0m, in \u001b[0;36mTrendReq.build_payload\u001b[1;34m(self, kw_list, cat, timeframe, geo, gprop)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_payload[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_payload[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# get tokens\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytrends\\request.py:195\u001b[0m, in \u001b[0;36mTrendReq._tokens\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Makes request to Google to get API tokens for interest over time, interest by region and related queries\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# make the request and parse the returned json\u001b[39;00m\n\u001b[1;32m--> 195\u001b[0m widget_dicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrendReq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGENERAL_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrendReq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOST_METHOD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrim_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidgets\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# order of the json matters...\u001b[39;00m\n\u001b[0;32m    202\u001b[0m first_region_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytrends\\request.py:136\u001b[0m, in \u001b[0;36mTrendReq._get_data\u001b[1;34m(self, url, method, trim_chars, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     s\u001b[38;5;241m.\u001b[39mproxies\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxies[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy_index]})\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m TrendReq\u001b[38;5;241m.\u001b[39mPOST_METHOD:\n\u001b[1;32m--> 136\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequests_args\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# DO NOT USE retries or backoff_factor here\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     response \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mget(url, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, cookies\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies,\n\u001b[0;32m    141\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequests_args)  \u001b[38;5;66;03m# DO NOT USE retries or backoff_factor here\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[1;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \n\u001b[0;32m    629\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:693\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    692\u001b[0m     sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 693\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    694\u001b[0m     server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[0;32m    695\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[0;32m     75\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from pytrends.request import TrendReq\n",
    "\n",
    "# Initialize Google Trends request object\n",
    "pytrends = TrendReq(hl=\"en-US\", tz=360)\n",
    "\n",
    "\n",
    "# Function to clean candidate name by removing special characters\n",
    "def isvalyti_kandidato_varda(vardas):\n",
    "    if isinstance(vardas, str):  # Ensure \"vardas\" is a string\n",
    "        return re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", vardas)\n",
    "    return \"\"  # Return empty string for non-string cases\n",
    "\n",
    "\n",
    "# Function to retrieve Google Trends data for a candidate in a specific year and state\n",
    "def gauti_paieskos_duomenis(kandidatas, state, year):\n",
    "    kandidatas = isvalyti_kandidato_varda(kandidatas)\n",
    "    start_date = f\"{year}-09-01\"\n",
    "    end_date = f\"{year}-10-31\"\n",
    "    try:\n",
    "        # Adding state to the keyword for better accuracy\n",
    "        search_term = f\"{kandidatas} {state}\"\n",
    "        pytrends.build_payload([search_term], timeframe=f\"{start_date} {end_date}\")\n",
    "        data = pytrends.interest_over_time()\n",
    "        if not data.empty:\n",
    "            return data[search_term].sum()  # Return total search interest over the specified period\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving data for candidate {kandidatas} ({state}, {year}): {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Path to the House data file\n",
    "failo_kelias = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\house.csv\"\n",
    "house_data = pd.read_csv(failo_kelias)\n",
    "\n",
    "# Filter the data to include only years 2004 and later\n",
    "house_data = house_data[house_data[\"year\"] >= 2004]\n",
    "\n",
    "# Add a column \"populiarumas_google\" if not already present\n",
    "if \"populiarumas_google\" not in house_data.columns:\n",
    "    house_data[\"populiarumas_google\"] = None\n",
    "\n",
    "# Extract unique candidate-state-year combinations after filtering\n",
    "unique_candidates = house_data[[\"candidate\", \"state\", \"year\"]].drop_duplicates()\n",
    "\n",
    "# Checkpoint path to save intermediate results\n",
    "checkpoint_path = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\house_checkpoint.csv\"\n",
    "\n",
    "# Load from checkpoint if it exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    house_data = pd.read_csv(checkpoint_path)\n",
    "    print(\"Loaded data from checkpoint.\")\n",
    "else:\n",
    "    print(\"Starting from scratch.\")\n",
    "\n",
    "# Batch processing parameters\n",
    "batch_size = 100  # Define how many records to process before saving a checkpoint\n",
    "batch_counter = 0  # Counter to track number of processed candidates\n",
    "pause_time = 5   # Fixed pause time in seconds to avoid rate limiting\n",
    "\n",
    "# Collect Google Trends data for unique candidates in each year and state\n",
    "for _, row in unique_candidates.iterrows():\n",
    "    kandidatas = row[\"candidate\"]\n",
    "    state = row[\"state\"]\n",
    "    year = row[\"year\"]\n",
    "\n",
    "    # Skip if data is already populated in checkpoint\n",
    "    if pd.notna(house_data.loc[(house_data[\"candidate\"] == kandidatas) &\n",
    "                               (house_data[\"state\"] == state) &\n",
    "                               (house_data[\"year\"] == year), \"populiarumas_google\"]).any():\n",
    "        continue\n",
    "\n",
    "    paieskos_kiekis = gauti_paieskos_duomenis(kandidatas, state, year)\n",
    "\n",
    "    # Save data by candidate, state, and year\n",
    "    house_data.loc[(house_data[\"candidate\"] == kandidatas) &\n",
    "                   (house_data[\"state\"] == state) &\n",
    "                   (house_data[\"year\"] == year), \"populiarumas_google\"] = paieskos_kiekis\n",
    "\n",
    "    print(f\"Processed candidate: {kandidatas} ({state}, {year}) - Popularity: {paieskos_kiekis}\")\n",
    "\n",
    "    # Increase counter and check if it\"s time to save the batch\n",
    "    batch_counter += 1\n",
    "    if batch_counter >= batch_size:\n",
    "        # Save checkpoint every 100 cases\n",
    "        house_data.to_csv(checkpoint_path, index=False)\n",
    "        print(f\"Saved checkpoint after processing {batch_counter} cases.\")\n",
    "        batch_counter = 0  # Reset counter\n",
    "\n",
    "    # Pause between requests to avoid hitting Google Trends rate limit\n",
    "    time.sleep(pause_time)\n",
    "\n",
    "# Save the final output\n",
    "naujas_failo_kelias = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\house_atnaujintas.csv\"\n",
    "house_data.to_csv(naujas_failo_kelias, index=False)\n",
    "print(\"Final data saved with the added \"populiarumas_google\" column!\")\n",
    "\n",
    "# Remove checkpoint after completion\n",
    "if os.path.exists(checkpoint_path):\n",
    "    os.remove(checkpoint_path)\n",
    "    print(\"Checkpoint file removed after completion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged House data with fully filled population column saved to C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paulius\\AppData\\Local\\Temp\\ipykernel_20876\\3434038282.py:34: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_data['population'] = merged_data['population'].fillna(method='ffill').fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load population data\n",
    "population_file_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\papildomi _duomenys\\Population Estimates - US, States, Counties.csv\"\n",
    "population_data = pd.read_csv(population_file_path)\n",
    "\n",
    "# Filter out U.S.-level data and aggregate county-level data to get state totals\n",
    "population_data = population_data[(population_data['Description'] != 'U.S.') & (population_data['Countyfips'] != '000')]\n",
    "\n",
    "# Aggregate county data to get total population for each state and year\n",
    "state_population_data = (population_data.groupby(['Description', 'Year'], as_index=False)\n",
    "                         .agg({'Population': 'sum'}))\n",
    "\n",
    "# Rename columns for merging\n",
    "state_population_data.columns = ['state', 'year', 'population']\n",
    "\n",
    "# Load House election data\n",
    "house_file_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\house.csv\"\n",
    "house_data = pd.read_csv(house_file_path)\n",
    "\n",
    "# Ensure consistent column types and case for merging\n",
    "house_data['year'] = house_data['year'].astype(int)\n",
    "state_population_data['year'] = state_population_data['year'].astype(int)\n",
    "house_data['state'] = house_data['state'].str.upper()\n",
    "state_population_data['state'] = state_population_data['state'].str.upper()\n",
    "\n",
    "# Merge House data with state population data on 'state' and 'year'\n",
    "merged_data = pd.merge(house_data, state_population_data, how='left', on=['state', 'year'])\n",
    "\n",
    "# Sort by state and year to ensure proper filling\n",
    "merged_data = merged_data.sort_values(['state', 'year']).reset_index(drop=True)\n",
    "\n",
    "# Apply forward and backward filling across the entire population column\n",
    "merged_data['population'] = merged_data['population'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Save the final merged data with filled population values\n",
    "output_file_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population.csv\"\n",
    "merged_data.to_csv(output_file_path, index=False)\n",
    "print(f\"Merged House data with fully filled population column saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Columns not found: 'Not Hispanic', 'Hawaiian or Pacific Islander Alone', 'Two or More Races', 'Hispanic', 'Black Alone', 'White Alone', 'American Indian or Alaskan Native', 'Total Population', 'Asian Alone'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m state_population_aggregated \u001b[38;5;241m=\u001b[39m state_population_aggregated\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_po\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Fill missing years within each state by forward-filling and then backward-filling\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m state_population_aggregated[numeric_columns] \u001b[38;5;241m=\u001b[39m \u001b[43mstate_population_aggregated\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate_po\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumeric_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m group: group\u001b[38;5;241m.\u001b[39mffill()\u001b[38;5;241m.\u001b[39mbfill())\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Load House data\u001b[39;00m\n\u001b[0;32m     51\u001b[0m house_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(house_data_path)\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:1951\u001b[0m, in \u001b[0;36mDataFrameGroupBy.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1945\u001b[0m     \u001b[38;5;66;03m# if len == 1, then it becomes a SeriesGroupBy and this is actually\u001b[39;00m\n\u001b[0;32m   1946\u001b[0m     \u001b[38;5;66;03m# valid syntax, so don't raise\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1948\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot subset columns with a tuple with more than one element. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1949\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1950\u001b[0m     )\n\u001b[1;32m-> 1951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:239\u001b[0m, in \u001b[0;36mSelectionMixin.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mintersection(key)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(key)):\n\u001b[0;32m    238\u001b[0m         bad_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(key)\u001b[38;5;241m.\u001b[39mdifference(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[1;32m--> 239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(bad_keys)[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gotitem(\u001b[38;5;28mlist\u001b[39m(key), ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Columns not found: 'Not Hispanic', 'Hawaiian or Pacific Islander Alone', 'Two or More Races', 'Hispanic', 'Black Alone', 'White Alone', 'American Indian or Alaskan Native', 'Total Population', 'Asian Alone'\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "population_race_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\papildomi _duomenys\\Population by Race - US, States, Counties.csv\"\n",
    "house_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house.csv\"\n",
    "output_house_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_race.csv\"\n",
    "\n",
    "# Load and prepare population data\n",
    "population_data = pd.read_csv(population_race_data_path)\n",
    "\n",
    "# Filter out U.S. totals, only keeping state and county data\n",
    "state_county_population_data = population_data[population_data['Description'] != 'U.S.'].copy()\n",
    "\n",
    "# Extract state abbreviations from Description using regex for consistency\n",
    "state_county_population_data['state_po'] = state_county_population_data['Description'].apply(\n",
    "    lambda desc: re.search(r', ([A-Z]{2})', desc).group(1) if re.search(r', ([A-Z]{2})', desc) else desc\n",
    ")\n",
    "\n",
    "# Convert relevant columns to numeric for aggregation\n",
    "numeric_columns = [\n",
    "    'Total Population', 'White Alone', 'Black Alone', 'American Indian or Alaskan Native',\n",
    "    'Asian Alone', 'Hawaiian or Pacific Islander Alone', 'Two or More Races', 'Not Hispanic', 'Hispanic'\n",
    "]\n",
    "state_county_population_data[numeric_columns] = state_county_population_data[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Group by 'Year' and 'state_po', summing the population values for each state\n",
    "state_population_aggregated = state_county_population_data.groupby(['Year', 'state_po'], as_index=False)[numeric_columns].sum()\n",
    "\n",
    "# Rename columns to Lithuanian as required\n",
    "state_population_aggregated = state_population_aggregated.rename(columns={\n",
    "    'Year': 'year',\n",
    "    'Total Population': 'Populiacija_viso',\n",
    "    'White Alone': 'Baltaodžiai',\n",
    "    'Black Alone': 'Juodaodžiai',\n",
    "    'American Indian or Alaskan Native': 'Indėnai_Arba_Aliaskos_vietiniai',\n",
    "    'Asian Alone': 'Azijiečiai',\n",
    "    'Hawaiian or Pacific Islander Alone': 'Havajiečiai',\n",
    "    'Two or More Races': 'Dvi_arba_daugiau_rasiu',\n",
    "    'Not Hispanic': 'Ne_ispanakalbiai',\n",
    "    'Hispanic': 'Ispanakalbiai'\n",
    "})\n",
    "\n",
    "# Sort by 'year' for each state to ensure forward and backward filling\n",
    "state_population_aggregated = state_population_aggregated.sort_values(['state_po', 'year']).reset_index(drop=True)\n",
    "\n",
    "# Fill missing years within each state by forward-filling and then backward-filling\n",
    "state_population_aggregated[numeric_columns] = state_population_aggregated.groupby('state_po')[numeric_columns].apply(lambda group: group.ffill().bfill())\n",
    "\n",
    "# Load House data\n",
    "house_data = pd.read_csv(house_data_path)\n",
    "\n",
    "# Merge the aggregated population data with the House data on 'year' and 'state_po'\n",
    "merged_house_data = pd.merge(\n",
    "    house_data,\n",
    "    state_population_aggregated,\n",
    "    on=['year', 'state_po'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Save the merged dataset to the specified output file\n",
    "merged_house_data.to_csv(output_house_path, index=False)\n",
    "print(f\"Updated House data with population by race saved to {output_house_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "incompatible index of inserted column with frame index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:12687\u001b[0m, in \u001b[0;36m_reindex_for_setitem\u001b[1;34m(value, index)\u001b[0m\n\u001b[0;32m  12686\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m> 12687\u001b[0m     reindexed_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m  12688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m  12689\u001b[0m     \u001b[38;5;66;03m# raised in MultiIndex.from_tuples, see test_insert_error_msmgs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:5153\u001b[0m, in \u001b[0;36mSeries.reindex\u001b[1;34m(self, index, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[0;32m   5136\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[0;32m   5137\u001b[0m     NDFrame\u001b[38;5;241m.\u001b[39mreindex,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m   5138\u001b[0m     klass\u001b[38;5;241m=\u001b[39m_shared_doc_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklass\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5151\u001b[0m     tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m-> 5153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:5610\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[1;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[0;32m   5609\u001b[0m \u001b[38;5;66;03m# perform the reindex on the axes\u001b[39;00m\n\u001b[1;32m-> 5610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_axes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5611\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m   5612\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:5633\u001b[0m, in \u001b[0;36mNDFrame._reindex_axes\u001b[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[0;32m   5632\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(a)\n\u001b[1;32m-> 5633\u001b[0m new_index, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\n\u001b[0;32m   5635\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5637\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(a)\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:4433\u001b[0m, in \u001b[0;36mIndex.reindex\u001b[1;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[0;32m   4431\u001b[0m             indexer, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(target)\n\u001b[1;32m-> 4433\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_reindex_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target, indexer\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:2717\u001b[0m, in \u001b[0;36mMultiIndex._wrap_reindex_result\u001b[1;34m(self, target, indexer, preserve_names)\u001b[0m\n\u001b[0;32m   2716\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2717\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[43mMultiIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tuples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   2719\u001b[0m     \u001b[38;5;66;03m# not all tuples, see test_constructor_dict_multiindex_reindex_flat\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:222\u001b[0m, in \u001b[0;36mnames_compat.<locals>.new_meth\u001b[1;34m(self_or_cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_or_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:617\u001b[0m, in \u001b[0;36mMultiIndex.from_tuples\u001b[1;34m(cls, tuples, sortorder, names)\u001b[0m\n\u001b[0;32m    615\u001b[0m         tuples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(tuples\u001b[38;5;241m.\u001b[39m_values)\n\u001b[1;32m--> 617\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuples_to_object_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtuples\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tuples, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32mlib.pyx:3029\u001b[0m, in \u001b[0;36mpandas._libs.lib.tuples_to_object_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Buffer dtype mismatch, expected 'Python object' but got 'long long'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20876\\533196952.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;31m# Sort by 'year' for each state to ensure forward and backward filling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[0mstate_population_aggregated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_population_aggregated\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state_po'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;31m# Fill missing years within each state by forward-filling and then backward-filling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m \u001b[0mstate_population_aggregated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfill_columns\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_population_aggregated\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'state_po'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfill_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mffill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;31m# Load House data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[0mhouse_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhouse_data_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4296\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ndim\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4297\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4298\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4299\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4300\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4301\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item_frame_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4302\u001b[0m         elif (\n",
      "\u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4340\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4341\u001b[0m                 \u001b[0mcheck_key_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4342\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4343\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4345\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4346\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4307\u001b[0m             \u001b[1;31m# Column to set is duplicated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4308\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4309\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4310\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4311\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4521\u001b[0m         \u001b[0mSeries\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mTimeSeries\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mconformed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mDataFrames\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4522\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4523\u001b[0m         \"\"\"\n\u001b[1;32m-> 4524\u001b[1;33m         \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4526\u001b[0m         if (\n\u001b[0;32m   4527\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5259\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5261\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5262\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5263\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_reindex_for_setitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5266\u001b[0m             \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(value, index)\u001b[0m\n\u001b[0;32m  12690\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12691\u001b[0m             \u001b[1;31m# duplicate axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12692\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 12694\u001b[1;33m         raise TypeError(\n\u001b[0m\u001b[0;32m  12695\u001b[0m             \u001b[1;34m\"incompatible index of inserted column with frame index\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12696\u001b[0m         \u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12697\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mreindexed_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: incompatible index of inserted column with frame index"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "population_race_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\papildomi _duomenys\\Population by Race - US, States, Counties.csv\"\n",
    "house_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house.csv\"\n",
    "output_house_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_race.csv\"\n",
    "\n",
    "# Load and prepare population data\n",
    "population_data = pd.read_csv(population_race_data_path)\n",
    "\n",
    "# Filter out U.S. totals, only keeping state and county data\n",
    "state_county_population_data = population_data[population_data['Description'] != 'U.S.'].copy()\n",
    "\n",
    "# Extract state abbreviations from Description using regex for consistency\n",
    "state_county_population_data['state_po'] = state_county_population_data['Description'].apply(\n",
    "    lambda desc: re.search(r', ([A-Z]{2})', desc).group(1) if re.search(r', ([A-Z]{2})', desc) else desc\n",
    ")\n",
    "\n",
    "# Convert relevant columns to numeric for aggregation (skip non-numeric fields)\n",
    "numeric_columns = [\n",
    "    'Total Population', 'White Alone', 'Black Alone', 'American Indian or Alaskan Native',\n",
    "    'Asian Alone', 'Hawaiian or Pacific Islander Alone', 'Two or More Races', 'Not Hispanic', 'Hispanic'\n",
    "]\n",
    "state_county_population_data[numeric_columns] = state_county_population_data[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Group by 'Year' and 'state_po', summing the population values for each state\n",
    "state_population_aggregated = state_county_population_data.groupby(['Year', 'state_po'], as_index=False)[numeric_columns].sum()\n",
    "\n",
    "# Rename columns to Lithuanian as required\n",
    "state_population_aggregated = state_population_aggregated.rename(columns={\n",
    "    'Year': 'year',\n",
    "    'Total Population': 'Populiacija_viso',\n",
    "    'White Alone': 'Baltaodžiai',\n",
    "    'Black Alone': 'Juodaodžiai',\n",
    "    'American Indian or Alaskan Native': 'Indėnai_Arba_Aliaskos_vietiniai',\n",
    "    'Asian Alone': 'Azijiečiai',\n",
    "    'Hawaiian or Pacific Islander Alone': 'Havajiečiai',\n",
    "    'Two or More Races': 'Dvi_arba_daugiau_rasiu',\n",
    "    'Not Hispanic': 'Ne_ispanakalbiai',\n",
    "    'Hispanic': 'Ispanakalbiai'\n",
    "})\n",
    "\n",
    "# Now, update the list of columns to fill to reflect the renamed columns\n",
    "fill_columns = [\n",
    "    'Populiacija_viso', 'Baltaodžiai', 'Juodaodžiai', \n",
    "    'Indėnai_Arba_Aliaskos_vietiniai', 'Azijiečiai', \n",
    "    'Havajiečiai', 'Dvi_arba_daugiau_rasiu', \n",
    "    'Ne_ispanakalbiai', 'Ispanakalbiai'\n",
    "]\n",
    "\n",
    "# Sort by 'year' for each state to ensure forward and backward filling\n",
    "state_population_aggregated = state_population_aggregated.sort_values(['state_po', 'year']).reset_index(drop=True)\n",
    "\n",
    "# Fill missing years within each state by forward-filling and then backward-filling\n",
    "state_population_aggregated[fill_columns] = state_population_aggregated.groupby('state_po')[fill_columns].apply(lambda group: group.ffill().bfill())\n",
    "\n",
    "# Load House data\n",
    "house_data = pd.read_csv(house_data_path)\n",
    "\n",
    "# Merge the aggregated population data with the House data on 'year' and 'state_po'\n",
    "merged_house_data = pd.merge(\n",
    "    house_data,\n",
    "    state_population_aggregated,\n",
    "    on=['year', 'state_po'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Save the merged dataset to the specified output file\n",
    "merged_house_data.to_csv(output_house_path, index=False)\n",
    "print(f\"Updated House data with population by race saved to {output_house_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated House data with population by race saved to C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_race.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "population_race_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\papildomi _duomenys\\Population by Race - US, States, Counties.csv\"\n",
    "house_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house.csv\"\n",
    "output_house_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_race.csv\"\n",
    "\n",
    "# Load and prepare population data\n",
    "population_data = pd.read_csv(population_race_data_path)\n",
    "\n",
    "# Filter out U.S. totals, only keeping state and county data\n",
    "state_county_population_data = population_data[population_data['Description'] != 'U.S.'].copy()\n",
    "\n",
    "# Extract state abbreviations from Description using regex for consistency\n",
    "state_county_population_data['state_po'] = state_county_population_data['Description'].apply(\n",
    "    lambda desc: re.search(r', ([A-Z]{2})', desc).group(1) if re.search(r', ([A-Z]{2})', desc) else desc\n",
    ")\n",
    "\n",
    "# Convert relevant columns to numeric for aggregation (skip non-numeric fields)\n",
    "numeric_columns = [\n",
    "    'Total Population', 'White Alone', 'Black Alone', 'American Indian or Alaskan Native',\n",
    "    'Asian Alone', 'Hawaiian or Pacific Islander Alone', 'Two or More Races', 'Not Hispanic', 'Hispanic'\n",
    "]\n",
    "state_county_population_data[numeric_columns] = state_county_population_data[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Group by 'Year' and 'state_po', summing the population values for each state\n",
    "state_population_aggregated = state_county_population_data.groupby(['Year', 'state_po'], as_index=False)[numeric_columns].sum()\n",
    "\n",
    "# Rename columns to Lithuanian as required\n",
    "state_population_aggregated = state_population_aggregated.rename(columns={\n",
    "    'Year': 'year',\n",
    "    'Total Population': 'Populiacija_viso',\n",
    "    'White Alone': 'Baltaodžiai',\n",
    "    'Black Alone': 'Juodaodžiai',\n",
    "    'American Indian or Alaskan Native': 'Indėnai_Arba_Aliaskos_vietiniai',\n",
    "    'Asian Alone': 'Azijiečiai',\n",
    "    'Hawaiian or Pacific Islander Alone': 'Havajiečiai',\n",
    "    'Two or More Races': 'Dvi_arba_daugiau_rasiu',\n",
    "    'Not Hispanic': 'Ne_ispanakalbiai',\n",
    "    'Hispanic': 'Ispanakalbiai'\n",
    "})\n",
    "\n",
    "# Define the columns to fill after renaming\n",
    "fill_columns = [\n",
    "    'Populiacija_viso', 'Baltaodžiai', 'Juodaodžiai', \n",
    "    'Indėnai_Arba_Aliaskos_vietiniai', 'Azijiečiai', \n",
    "    'Havajiečiai', 'Dvi_arba_daugiau_rasiu', \n",
    "    'Ne_ispanakalbiai', 'Ispanakalbiai'\n",
    "]\n",
    "\n",
    "# Sort by 'year' for each state to ensure proper fill sequence\n",
    "state_population_aggregated = state_population_aggregated.sort_values(['state_po', 'year']).reset_index(drop=True)\n",
    "\n",
    "# Fill missing years within each state separately and update\n",
    "for state, group in state_population_aggregated.groupby('state_po'):\n",
    "    filled_group = group[fill_columns].ffill().bfill()\n",
    "    state_population_aggregated.update(filled_group)\n",
    "\n",
    "# Load House data\n",
    "house_data = pd.read_csv(house_data_path)\n",
    "\n",
    "# Merge the filled population data with the House data on 'year' and 'state_po'\n",
    "merged_house_data = pd.merge(\n",
    "    house_data,\n",
    "    state_population_aggregated,\n",
    "    on=['year', 'state_po'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Save the merged dataset to the specified output file\n",
    "merged_house_data.to_csv(output_house_path, index=False)\n",
    "print(f\"Updated House data with population by race saved to {output_house_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House data with fully filled population data saved to C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_race_filled_complete.csv\n"
     ]
    }
   ],
   "source": [
    "# Apply forward fill and backward fill within each state to handle any remaining missing values after merging\n",
    "merged_house_data = merged_house_data.sort_values(['state_po', 'year']).reset_index(drop=True)\n",
    "\n",
    "# Fill missing values for population-related columns within each state\n",
    "for state, group in merged_house_data.groupby('state_po'):\n",
    "    # Fill missing values within each state's group using forward fill first, then backward fill\n",
    "    group[fill_columns] = group[fill_columns].ffill().bfill()\n",
    "    merged_house_data.update(group)\n",
    "\n",
    "# Save the fully filled data to a new output file\n",
    "filled_output_house_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_race_filled_complete.csv\"\n",
    "merged_house_data.to_csv(filled_output_house_path, index=False)\n",
    "print(f\"House data with fully filled population data saved to {filled_output_house_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Columns not found: 'Female Population', 'Population 5-17', 'Male Population', 'Population 18-24', 'Population Under 18', 'Population 25-44', 'Population 65+', 'Population 0-4', 'Population 55+', 'Population 45-64', 'Total Population', 'Population 18-54'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m state_population_aggregated \u001b[38;5;241m=\u001b[39m state_population_aggregated\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_po\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Fill missing years for each state\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m state_population_aggregated[numeric_columns] \u001b[38;5;241m=\u001b[39m \u001b[43mstate_population_aggregated\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate_po\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumeric_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m group: group\u001b[38;5;241m.\u001b[39mffill()\u001b[38;5;241m.\u001b[39mbfill())\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Load House data\u001b[39;00m\n\u001b[0;32m     56\u001b[0m house_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(house_data_path)\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:1951\u001b[0m, in \u001b[0;36mDataFrameGroupBy.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1945\u001b[0m     \u001b[38;5;66;03m# if len == 1, then it becomes a SeriesGroupBy and this is actually\u001b[39;00m\n\u001b[0;32m   1946\u001b[0m     \u001b[38;5;66;03m# valid syntax, so don't raise\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1948\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot subset columns with a tuple with more than one element. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1949\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1950\u001b[0m     )\n\u001b[1;32m-> 1951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:239\u001b[0m, in \u001b[0;36mSelectionMixin.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mintersection(key)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(key)):\n\u001b[0;32m    238\u001b[0m         bad_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(key)\u001b[38;5;241m.\u001b[39mdifference(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[1;32m--> 239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(bad_keys)[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gotitem(\u001b[38;5;28mlist\u001b[39m(key), ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Columns not found: 'Female Population', 'Population 5-17', 'Male Population', 'Population 18-24', 'Population Under 18', 'Population 25-44', 'Population 65+', 'Population 0-4', 'Population 55+', 'Population 45-64', 'Total Population', 'Population 18-54'\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "age_sex_population_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\papildomi _duomenys\\Population by Age and Sex - US, States, Counties.csv\"\n",
    "house_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house.csv\"\n",
    "output_house_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_age_sex.csv\"\n",
    "\n",
    "# Load and prepare population data\n",
    "age_sex_population_data = pd.read_csv(age_sex_population_data_path)\n",
    "\n",
    "# Filter to keep only state-level data (Countyfips == '000') and exclude U.S. totals\n",
    "state_population_data = age_sex_population_data[(age_sex_population_data['Countyfips'] == '000') & \n",
    "                                                (age_sex_population_data['Description'] != 'U.S.')].copy()\n",
    "\n",
    "# Extract state abbreviations for consistent merging\n",
    "state_population_data['state_po'] = state_population_data['Description'].apply(\n",
    "    lambda desc: re.search(r', ([A-Z]{2})', desc).group(1) if re.search(r', ([A-Z]{2})', desc) else desc\n",
    ")\n",
    "\n",
    "# Convert numeric columns to appropriate data types\n",
    "numeric_columns = [\n",
    "    'Total Population', 'Population 0-4', 'Population 5-17', 'Population 18-24',\n",
    "    'Population 25-44', 'Population 45-64', 'Population 65+', 'Population Under 18', \n",
    "    'Population 18-54', 'Population 55+', 'Male Population', 'Female Population'\n",
    "]\n",
    "state_population_data[numeric_columns] = state_population_data[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Group by 'Year' and 'state_po' to sum population metrics at the state level\n",
    "state_population_aggregated = state_population_data.groupby(['Year', 'state_po'], as_index=False)[numeric_columns].sum()\n",
    "\n",
    "# Rename columns for consistency with Lithuanian names\n",
    "state_population_aggregated = state_population_aggregated.rename(columns={\n",
    "    'Year': 'year',\n",
    "    'Total Population': 'Populiacija_viso',\n",
    "    'Population 0-4': 'Populiacija_0_4',\n",
    "    'Population 5-17': 'Populiacija_5_17',\n",
    "    'Population 18-24': 'Populiacija_18_24',\n",
    "    'Population 25-44': 'Populiacija_25_44',\n",
    "    'Population 45-64': 'Populiacija_45_64',\n",
    "    'Population 65+': 'Populiacija_65_plus',\n",
    "    'Population Under 18': 'Populiacija_under_18',\n",
    "    'Population 18-54': 'Populiacija_18_54',\n",
    "    'Population 55+': 'Populiacija_55_plus',\n",
    "    'Male Population': 'Vyru_populiacija',\n",
    "    'Female Population': 'Moteru_populiacija'\n",
    "})\n",
    "\n",
    "# Sort by 'year' for each state to enable forward and backward filling\n",
    "state_population_aggregated = state_population_aggregated.sort_values(['state_po', 'year']).reset_index(drop=True)\n",
    "\n",
    "# Fill missing years for each state\n",
    "state_population_aggregated[numeric_columns] = state_population_aggregated.groupby('state_po')[numeric_columns].apply(lambda group: group.ffill().bfill())\n",
    "\n",
    "# Load House data\n",
    "house_data = pd.read_csv(house_data_path)\n",
    "\n",
    "# Merge the aggregated population data with the House data\n",
    "merged_house_data = pd.merge(\n",
    "    house_data,\n",
    "    state_population_aggregated,\n",
    "    on=['year', 'state_po'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Sort merged data for consistency and apply forward and backward filling within each state\n",
    "merged_house_data = merged_house_data.sort_values(['state_po', 'year']).reset_index(drop=True)\n",
    "for state, group in merged_house_data.groupby('state_po'):\n",
    "    group[numeric_columns] = group[numeric_columns].ffill().bfill()\n",
    "    merged_house_data.update(group)\n",
    "\n",
    "# Save the filled data to a new file\n",
    "merged_house_data.to_csv(output_house_path, index=False)\n",
    "print(f\"House data with age and sex population information saved to {output_house_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House data with age and sex population information saved to C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_age_sex.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "age_sex_population_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\papildomi _duomenys\\Population by Age and Sex - US, States, Counties.csv\"\n",
    "house_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house.csv\"\n",
    "output_house_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_age_sex.csv\"\n",
    "\n",
    "# Load and prepare population data\n",
    "age_sex_population_data = pd.read_csv(age_sex_population_data_path)\n",
    "\n",
    "# Filter to keep only state-level data (Countyfips == '000') and exclude U.S. totals\n",
    "state_population_data = age_sex_population_data[(age_sex_population_data['Countyfips'] == '000') & \n",
    "                                                (age_sex_population_data['Description'] != 'U.S.')].copy()\n",
    "\n",
    "# Extract state abbreviations for consistent merging\n",
    "state_population_data['state_po'] = state_population_data['Description'].apply(\n",
    "    lambda desc: re.search(r', ([A-Z]{2})', desc).group(1) if re.search(r', ([A-Z]{2})', desc) else desc\n",
    ")\n",
    "\n",
    "# Convert relevant columns to numeric\n",
    "numeric_columns_original = [\n",
    "    'Total Population', 'Population 0-4', 'Population 5-17', 'Population 18-24',\n",
    "    'Population 25-44', 'Population 45-64', 'Population 65+', 'Population Under 18', \n",
    "    'Population 18-54', 'Population 55+', 'Male Population', 'Female Population'\n",
    "]\n",
    "state_population_data[numeric_columns_original] = state_population_data[numeric_columns_original].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Group by 'Year' and 'state_po' to sum population metrics at the state level\n",
    "state_population_aggregated = state_population_data.groupby(['Year', 'state_po'], as_index=False)[numeric_columns_original].sum()\n",
    "\n",
    "# Rename columns to Lithuanian names as required\n",
    "rename_mapping = {\n",
    "    'Year': 'year',\n",
    "    'Total Population': 'Populiacija_viso',\n",
    "    'Population 0-4': 'Populiacija_0_4',\n",
    "    'Population 5-17': 'Populiacija_5_17',\n",
    "    'Population 18-24': 'Populiacija_18_24',\n",
    "    'Population 25-44': 'Populiacija_25_44',\n",
    "    'Population 45-64': 'Populiacija_45_64',\n",
    "    'Population 65+': 'Populiacija_65_plus',\n",
    "    'Population Under 18': 'Populiacija_under_18',\n",
    "    'Population 18-54': 'Populiacija_18_54',\n",
    "    'Population 55+': 'Populiacija_55_plus',\n",
    "    'Male Population': 'Vyru_populiacija',\n",
    "    'Female Population': 'Moteru_populiacija'\n",
    "}\n",
    "state_population_aggregated = state_population_aggregated.rename(columns=rename_mapping)\n",
    "\n",
    "# Update `numeric_columns` to use renamed columns\n",
    "numeric_columns = list(rename_mapping.values())[1:]  # Exclude 'year'\n",
    "\n",
    "# Ensure sorting for forward and backward filling\n",
    "state_population_aggregated = state_population_aggregated.sort_values(['state_po', 'year']).reset_index(drop=True)\n",
    "\n",
    "# Fill missing years for each state by forward- and backward-filling\n",
    "state_population_aggregated[numeric_columns] = state_population_aggregated.groupby('state_po')[numeric_columns].apply(lambda group: group.ffill().bfill())\n",
    "\n",
    "# Load House data\n",
    "house_data = pd.read_csv(house_data_path)\n",
    "\n",
    "# Merge the aggregated population data with the House data\n",
    "merged_house_data = pd.merge(\n",
    "    house_data,\n",
    "    state_population_aggregated,\n",
    "    on=['year', 'state_po'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Re-check for columns to fill after merge\n",
    "existing_fill_columns = [col for col in numeric_columns if col in merged_house_data.columns]\n",
    "\n",
    "# Sort merged data and apply forward/backward fill within each state\n",
    "merged_house_data = merged_house_data.sort_values(['state_po', 'year']).reset_index(drop=True)\n",
    "for state, group in merged_house_data.groupby('state_po'):\n",
    "    group[existing_fill_columns] = group[existing_fill_columns].ffill().bfill()\n",
    "    merged_house_data.update(group)\n",
    "\n",
    "# Save the filled data to a new file\n",
    "merged_house_data.to_csv(output_house_path, index=False)\n",
    "print(f\"House data with age and sex population information saved to {output_house_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House data with age and sex population information saved to C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_age_sex.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "age_sex_population_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\papildomi _duomenys\\Population by Age and Sex - US, States, Counties.csv\"\n",
    "house_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house.csv\"\n",
    "output_house_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_age_sex.csv\"\n",
    "\n",
    "# Load and prepare population data\n",
    "age_sex_population_data = pd.read_csv(age_sex_population_data_path)\n",
    "\n",
    "# Filter to keep only state-level data (Countyfips == '000') and exclude U.S. totals\n",
    "state_population_data = age_sex_population_data[\n",
    "    (age_sex_population_data['Countyfips'] == '000') & \n",
    "    (~age_sex_population_data['Description'].str.contains('U.S.', case=False))\n",
    "].copy()\n",
    "\n",
    "# Extract state abbreviations or state names from `Description`\n",
    "state_population_data['state_po'] = state_population_data['Description'].apply(\n",
    "    lambda desc: re.search(r', ([A-Z]{2})', desc).group(1) if re.search(r', ([A-Z]{2})', desc) else desc.strip()\n",
    ")\n",
    "\n",
    "# Convert relevant columns to numeric for aggregation\n",
    "numeric_columns_original = [\n",
    "    'Total Population', 'Population 0-4', 'Population 5-17', 'Population 18-24',\n",
    "    'Population 25-44', 'Population 45-64', 'Population 65+', 'Population Under 18', \n",
    "    'Population 18-54', 'Population 55+', 'Male Population', 'Female Population'\n",
    "]\n",
    "state_population_data[numeric_columns_original] = state_population_data[numeric_columns_original].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Group by 'Year' and 'state_po' to sum population metrics at the state level\n",
    "state_population_aggregated = state_population_data.groupby(['Year', 'state_po'], as_index=False)[numeric_columns_original].sum()\n",
    "\n",
    "# Rename columns to Lithuanian as required\n",
    "rename_mapping = {\n",
    "    'Year': 'year',\n",
    "    'Total Population': 'Populiacija_viso',\n",
    "    'Population 0-4': 'Populiacija_0_4',\n",
    "    'Population 5-17': 'Populiacija_5_17',\n",
    "    'Population 18-24': 'Populiacija_18_24',\n",
    "    'Population 25-44': 'Populiacija_25_44',\n",
    "    'Population 45-64': 'Populiacija_45_64',\n",
    "    'Population 65+': 'Populiacija_65_plus',\n",
    "    'Population Under 18': 'Populiacija_under_18',\n",
    "    'Population 18-54': 'Populiacija_18_54',\n",
    "    'Population 55+': 'Populiacija_55_plus',\n",
    "    'Male Population': 'Vyru_populiacija',\n",
    "    'Female Population': 'Moteru_populiacija'\n",
    "}\n",
    "state_population_aggregated = state_population_aggregated.rename(columns=rename_mapping)\n",
    "\n",
    "# Update `numeric_columns` to use renamed columns\n",
    "numeric_columns = list(rename_mapping.values())[1:]  # Exclude 'year'\n",
    "\n",
    "# Ensure sorting for forward and backward filling\n",
    "state_population_aggregated = state_population_aggregated.sort_values(['state_po', 'year']).reset_index(drop=True)\n",
    "\n",
    "# Fill missing years for each state by forward- and backward-filling\n",
    "state_population_aggregated[numeric_columns] = state_population_aggregated.groupby('state_po')[numeric_columns].apply(lambda group: group.ffill().bfill())\n",
    "\n",
    "# Load House data\n",
    "house_data = pd.read_csv(house_data_path)\n",
    "\n",
    "# Merge the aggregated population data with the House data\n",
    "merged_house_data = pd.merge(\n",
    "    house_data,\n",
    "    state_population_aggregated,\n",
    "    on=['year', 'state_po'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Re-check for columns to fill after merge\n",
    "existing_fill_columns = [col for col in numeric_columns if col in merged_house_data.columns]\n",
    "\n",
    "# Sort merged data and apply forward/backward fill within each state\n",
    "merged_house_data = merged_house_data.sort_values(['state_po', 'year']).reset_index(drop=True)\n",
    "for state, group in merged_house_data.groupby('state_po'):\n",
    "    group[existing_fill_columns] = group[existing_fill_columns].ffill().bfill()\n",
    "    merged_house_data.update(group)\n",
    "\n",
    "# Save the filled data to a new file\n",
    "merged_house_data.to_csv(output_house_path, index=False)\n",
    "print(f\"House data with age and sex population information saved to {output_house_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned House data with filled population data saved to C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your merged House data file with the new population data\n",
    "merged_house_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_age_sex.csv\"\n",
    "output_house_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_cleaned.csv\"\n",
    "\n",
    "# Load data\n",
    "merged_house_data = pd.read_csv(merged_house_data_path)\n",
    "\n",
    "# Consolidate duplicate population columns (`Populiacija_viso_x` and `Populiacija_viso_y`)\n",
    "merged_house_data['Populiacija_viso'] = merged_house_data['Populiacija_viso_x'].combine_first(merged_house_data['Populiacija_viso_y'])\n",
    "\n",
    "# Drop the original `_x` and `_y` columns to avoid redundancy\n",
    "merged_house_data = merged_house_data.drop(columns=['Populiacija_viso_x', 'Populiacija_viso_y'])\n",
    "\n",
    "# List of population-related columns to ensure consistent filling\n",
    "population_columns = [\n",
    "    'Populiacija_viso', 'Baltaodžiai', 'Juodaodžiai', \n",
    "    'Indėnai_Arba_Aliaskos_vietiniai', 'Azijiečiai', 'Havajiečiai', \n",
    "    'Dvi_arba_daugiau_rasiu', 'Ne_ispanakalbiai', 'Ispanakalbiai',\n",
    "    'Populiacija_0_4', 'Populiacija_5_17', 'Populiacija_18_24', \n",
    "    'Populiacija_25_44', 'Populiacija_45_64', 'Populiacija_65_plus', \n",
    "    'Populiacija_under_18', 'Populiacija_18_54', 'Populiacija_55_plus', \n",
    "    'Vyru_populiacija', 'Moteru_populiacija'\n",
    "]\n",
    "\n",
    "# Sort data by `state_po` and `year` for consistent state-wise filling\n",
    "merged_house_data = merged_house_data.sort_values(['state_po', 'year']).reset_index(drop=True)\n",
    "\n",
    "# Fill missing population data within each state\n",
    "for state, group in merged_house_data.groupby('state_po'):\n",
    "    group[population_columns] = group[population_columns].ffill().bfill()\n",
    "    merged_house_data.update(group)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "merged_house_data.to_csv(output_house_path, index=False)\n",
    "print(f\"Cleaned House data with filled population data saved to {output_house_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty cells remaining in columns after filling:\n",
      "Baltaodžiai                           13\n",
      "Juodaodžiai                           13\n",
      "Indėnai_Arba_Aliaskos_vietiniai       13\n",
      "Azijiečiai                            13\n",
      "Havajiečiai                           13\n",
      "Dvi_arba_daugiau_rasiu                13\n",
      "Ne_ispanakalbiai                      13\n",
      "Ispanakalbiai                         13\n",
      "Populiacija_0_4                    32452\n",
      "Populiacija_5_17                   32452\n",
      "Populiacija_18_24                  32452\n",
      "Populiacija_25_44                  32452\n",
      "Populiacija_45_64                  32452\n",
      "Populiacija_65_plus                32452\n",
      "Populiacija_under_18               32452\n",
      "Populiacija_18_54                  32452\n",
      "Populiacija_55_plus                32452\n",
      "Vyru_populiacija                   32452\n",
      "Moteru_populiacija                 32452\n",
      "dtype: int64\n",
      "Cleaned House data with filled population columns saved to C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "merged_house_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_age_sex.csv\"\n",
    "output_house_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_cleaned.csv\"\n",
    "\n",
    "# Load the House data with the new population data\n",
    "merged_house_data = pd.read_csv(merged_house_data_path)\n",
    "\n",
    "# List of specific population columns to be filled, excluding `Populiacija_viso`\n",
    "population_columns = [\n",
    "    'Baltaodžiai', 'Juodaodžiai', 'Indėnai_Arba_Aliaskos_vietiniai', \n",
    "    'Azijiečiai', 'Havajiečiai', 'Dvi_arba_daugiau_rasiu', \n",
    "    'Ne_ispanakalbiai', 'Ispanakalbiai', 'Populiacija_0_4', \n",
    "    'Populiacija_5_17', 'Populiacija_18_24', 'Populiacija_25_44', \n",
    "    'Populiacija_45_64', 'Populiacija_65_plus', 'Populiacija_under_18', \n",
    "    'Populiacija_18_54', 'Populiacija_55_plus', 'Vyru_populiacija', \n",
    "    'Moteru_populiacija'\n",
    "]\n",
    "\n",
    "# Sort by `state_po` and `year` to prepare for forward and backward filling\n",
    "merged_house_data = merged_house_data.sort_values(['state_po', 'year']).reset_index(drop=True)\n",
    "\n",
    "# Ensure that all specified population columns exist in the data, avoiding KeyErrors\n",
    "existing_columns = [col for col in population_columns if col in merged_house_data.columns]\n",
    "\n",
    "# Fill missing data in population-related columns within each state\n",
    "for state, group in merged_house_data.groupby('state_po'):\n",
    "    group[existing_columns] = group[existing_columns].ffill().bfill()\n",
    "    merged_house_data.update(group)\n",
    "\n",
    "# Check for any remaining empty cells in the specified columns and print feedback\n",
    "empty_cells = merged_house_data[existing_columns].isnull().sum()\n",
    "print(\"Empty cells remaining in columns after filling:\")\n",
    "print(empty_cells[empty_cells > 0])  # Shows any columns that still have empty cells\n",
    "\n",
    "# Save the cleaned data without `Populiacija_viso`\n",
    "merged_house_data.to_csv(output_house_path, index=False)\n",
    "print(f\"Cleaned House data with filled population columns saved to {output_house_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "left keys must be sorted",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m state_population_aggregated \u001b[38;5;241m=\u001b[39m state_population_aggregated\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_po\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Merge state-level population data, matching to the closest available year for each election year\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m merged_house_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_asof\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhouse_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_population_aggregated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate_po\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnearest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Matches the closest previous year if available, otherwise the next closest year\u001b[39;49;00m\n\u001b[0;32m     63\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Save the merged data to a new file\u001b[39;00m\n\u001b[0;32m     66\u001b[0m merged_house_data\u001b[38;5;241m.\u001b[39mto_csv(output_house_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:708\u001b[0m, in \u001b[0;36mmerge_asof\u001b[1;34m(left, right, on, left_on, right_on, left_index, right_index, by, left_by, right_by, suffixes, tolerance, allow_exact_matches, direction)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;124;03mPerform a merge by key distance.\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;124;03m4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    691\u001b[0m op \u001b[38;5;241m=\u001b[39m _AsOfMerge(\n\u001b[0;32m    692\u001b[0m     left,\n\u001b[0;32m    693\u001b[0m     right,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    706\u001b[0m     direction\u001b[38;5;241m=\u001b[39mdirection,\n\u001b[0;32m    707\u001b[0m )\n\u001b[1;32m--> 708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1926\u001b[0m, in \u001b[0;36m_OrderedMerge.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m-> 1926\u001b[0m     join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1928\u001b[0m     left_join_indexer: npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1929\u001b[0m     right_join_indexer: npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1151\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     join_index, right_indexer, left_indexer \u001b[38;5;241m=\u001b[39m _left_join_on_index(\n\u001b[0;32m   1148\u001b[0m         right_ax, left_ax, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1151\u001b[0m     (left_indexer, right_indexer) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_index:\n\u001b[0;32m   1154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:2238\u001b[0m, in \u001b[0;36m_AsOfMerge._get_join_indexers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2235\u001b[0m         tolerance \u001b[38;5;241m=\u001b[39m tolerance\u001b[38;5;241m.\u001b[39m_value\n\u001b[0;32m   2237\u001b[0m \u001b[38;5;66;03m# initial type conversion as needed\u001b[39;00m\n\u001b[1;32m-> 2238\u001b[0m left_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_values_for_libjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2239\u001b[0m right_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_values_for_libjoin(right_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2241\u001b[0m \u001b[38;5;66;03m# a \"by\" parameter requires special handling\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:2182\u001b[0m, in \u001b[0;36m_AsOfMerge._convert_values_for_libjoin\u001b[1;34m(self, values, side)\u001b[0m\n\u001b[0;32m   2180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isna(values)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   2181\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerge keys contain null values on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mside\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m side\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mside\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m keys must be sorted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, ArrowExtensionArray):\n\u001b[0;32m   2185\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39m_maybe_convert_datelike_array()\n",
      "\u001b[1;31mValueError\u001b[0m: left keys must be sorted"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "population_age_sex_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\papildomi _duomenys\\Population by Age and Sex - US, States, Counties.csv\"\n",
    "house_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house.csv\"\n",
    "output_house_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_age_sex.csv\"\n",
    "\n",
    "# Load Population data\n",
    "population_data = pd.read_csv(population_age_sex_data_path)\n",
    "\n",
    "# Filter to exclude U.S.-level data, keeping only states and counties\n",
    "state_county_population_data = population_data[population_data['Description'] != 'U.S.'].copy()\n",
    "\n",
    "# Extract state abbreviations\n",
    "state_county_population_data['state_po'] = state_county_population_data['Description'].apply(\n",
    "    lambda desc: re.search(r', ([A-Z]{2})', desc).group(1) if re.search(r', ([A-Z]{2})', desc) else desc\n",
    ")\n",
    "\n",
    "# Define the population columns to sum\n",
    "age_sex_columns = [\n",
    "    'Total Population', 'Population 0-4', 'Population 5-17', 'Population 18-24', \n",
    "    'Population 25-44', 'Population 45-64', 'Population 65+', 'Population Under 18', \n",
    "    'Population 18-54', 'Population 55+', 'Male Population', 'Female Population'\n",
    "]\n",
    "\n",
    "# Convert relevant columns to numeric (some may contain non-numeric data)\n",
    "state_county_population_data[age_sex_columns] = state_county_population_data[age_sex_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Group by 'Year' and 'state_po' to aggregate county data into state-level totals\n",
    "state_population_aggregated = state_county_population_data.groupby(['Year', 'state_po'], as_index=False)[age_sex_columns].sum()\n",
    "\n",
    "# Rename columns to Lithuanian as required\n",
    "state_population_aggregated = state_population_aggregated.rename(columns={\n",
    "    'Year': 'year',\n",
    "    'Population 0-4': 'Populiacija_0_4',\n",
    "    'Population 5-17': 'Populiacija_5_17',\n",
    "    'Population 18-24': 'Populiacija_18_24',\n",
    "    'Population 25-44': 'Populiacija_25_44',\n",
    "    'Population 45-64': 'Populiacija_45_64',\n",
    "    'Population 65+': 'Populiacija_65_plus',\n",
    "    'Population Under 18': 'Populiacija_under_18',\n",
    "    'Population 18-54': 'Populiacija_18_54',\n",
    "    'Population 55+': 'Populiacija_55_plus',\n",
    "    'Male Population': 'Vyru_populiacija',\n",
    "    'Female Population': 'Moteru_populiacija'\n",
    "})\n",
    "\n",
    "# Load House election data\n",
    "house_data = pd.read_csv(house_data_path)\n",
    "\n",
    "# Sort and merge using asof to handle non-matching years (nearest match)\n",
    "house_data = house_data.sort_values(['state_po', 'year']).reset_index(drop=True)\n",
    "state_population_aggregated = state_population_aggregated.sort_values(['state_po', 'year']).reset_index(drop=True)\n",
    "\n",
    "# Merge state-level population data, matching to the closest available year for each election year\n",
    "merged_house_data = pd.merge_asof(\n",
    "    house_data,\n",
    "    state_population_aggregated,\n",
    "    on='year',\n",
    "    by='state_po',\n",
    "    direction='nearest',  # Matches the closest previous year if available, otherwise the next closest year\n",
    ")\n",
    "\n",
    "# Save the merged data to a new file\n",
    "merged_house_data.to_csv(output_house_path, index=False)\n",
    "print(f\"House data with population by age and sex saved to {output_house_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "left keys must be sorted",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m state_population_aggregated \u001b[38;5;241m=\u001b[39m state_population_aggregated\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_po\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Merge population data with House election data, matching each election year to the closest available population data year\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m merged_house_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_asof\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhouse_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_population_aggregated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate_po\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnearest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Matches the closest available year for each election year\u001b[39;49;00m\n\u001b[0;32m     63\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Save the merged dataset\u001b[39;00m\n\u001b[0;32m     66\u001b[0m merged_house_data\u001b[38;5;241m.\u001b[39mto_csv(output_house_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:708\u001b[0m, in \u001b[0;36mmerge_asof\u001b[1;34m(left, right, on, left_on, right_on, left_index, right_index, by, left_by, right_by, suffixes, tolerance, allow_exact_matches, direction)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;124;03mPerform a merge by key distance.\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;124;03m4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    691\u001b[0m op \u001b[38;5;241m=\u001b[39m _AsOfMerge(\n\u001b[0;32m    692\u001b[0m     left,\n\u001b[0;32m    693\u001b[0m     right,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    706\u001b[0m     direction\u001b[38;5;241m=\u001b[39mdirection,\n\u001b[0;32m    707\u001b[0m )\n\u001b[1;32m--> 708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1926\u001b[0m, in \u001b[0;36m_OrderedMerge.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m-> 1926\u001b[0m     join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1928\u001b[0m     left_join_indexer: npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1929\u001b[0m     right_join_indexer: npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1151\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     join_index, right_indexer, left_indexer \u001b[38;5;241m=\u001b[39m _left_join_on_index(\n\u001b[0;32m   1148\u001b[0m         right_ax, left_ax, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1151\u001b[0m     (left_indexer, right_indexer) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_index:\n\u001b[0;32m   1154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:2238\u001b[0m, in \u001b[0;36m_AsOfMerge._get_join_indexers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2235\u001b[0m         tolerance \u001b[38;5;241m=\u001b[39m tolerance\u001b[38;5;241m.\u001b[39m_value\n\u001b[0;32m   2237\u001b[0m \u001b[38;5;66;03m# initial type conversion as needed\u001b[39;00m\n\u001b[1;32m-> 2238\u001b[0m left_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_values_for_libjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2239\u001b[0m right_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_values_for_libjoin(right_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2241\u001b[0m \u001b[38;5;66;03m# a \"by\" parameter requires special handling\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:2182\u001b[0m, in \u001b[0;36m_AsOfMerge._convert_values_for_libjoin\u001b[1;34m(self, values, side)\u001b[0m\n\u001b[0;32m   2180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isna(values)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   2181\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerge keys contain null values on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mside\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m side\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mside\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m keys must be sorted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, ArrowExtensionArray):\n\u001b[0;32m   2185\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39m_maybe_convert_datelike_array()\n",
      "\u001b[1;31mValueError\u001b[0m: left keys must be sorted"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "population_age_sex_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\papildomi _duomenys\\Population by Age and Sex - US, States, Counties.csv\"\n",
    "house_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house.csv\"\n",
    "output_house_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_age_sex.csv\"\n",
    "\n",
    "# Load Population data\n",
    "population_data = pd.read_csv(population_age_sex_data_path)\n",
    "\n",
    "# Filter out U.S. totals, keeping only state and county data\n",
    "state_county_population_data = population_data[population_data['Description'] != 'U.S.'].copy()\n",
    "\n",
    "# Extract state abbreviations for consistency\n",
    "state_county_population_data['state_po'] = state_county_population_data['Description'].apply(\n",
    "    lambda desc: re.search(r', ([A-Z]{2})', desc).group(1) if re.search(r', ([A-Z]{2})', desc) else desc\n",
    ")\n",
    "\n",
    "# Define population columns to aggregate\n",
    "age_sex_columns = [\n",
    "    'Total Population', 'Population 0-4', 'Population 5-17', 'Population 18-24', \n",
    "    'Population 25-44', 'Population 45-64', 'Population 65+', 'Population Under 18', \n",
    "    'Population 18-54', 'Population 55+', 'Male Population', 'Female Population'\n",
    "]\n",
    "\n",
    "# Convert columns to numeric for proper aggregation\n",
    "state_county_population_data[age_sex_columns] = state_county_population_data[age_sex_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Aggregate data at the state level by summing county values within each state and year\n",
    "state_population_aggregated = state_county_population_data.groupby(['Year', 'state_po'], as_index=False)[age_sex_columns].sum()\n",
    "\n",
    "# Rename columns to Lithuanian equivalents\n",
    "state_population_aggregated = state_population_aggregated.rename(columns={\n",
    "    'Year': 'year',\n",
    "    'Population 0-4': 'Populiacija_0_4',\n",
    "    'Population 5-17': 'Populiacija_5_17',\n",
    "    'Population 18-24': 'Populiacija_18_24',\n",
    "    'Population 25-44': 'Populiacija_25_44',\n",
    "    'Population 45-64': 'Populiacija_45_64',\n",
    "    'Population 65+': 'Populiacija_65_plus',\n",
    "    'Population Under 18': 'Populiacija_under_18',\n",
    "    'Population 18-54': 'Populiacija_18_54',\n",
    "    'Population 55+': 'Populiacija_55_plus',\n",
    "    'Male Population': 'Vyru_populiacija',\n",
    "    'Female Population': 'Moteru_populiacija'\n",
    "})\n",
    "\n",
    "# Load House election data\n",
    "house_data = pd.read_csv(house_data_path)\n",
    "\n",
    "# Sort both datasets for merging\n",
    "house_data = house_data.sort_values(['state_po', 'year']).reset_index(drop=True)\n",
    "state_population_aggregated = state_population_aggregated.sort_values(['state_po', 'year']).reset_index(drop=True)\n",
    "\n",
    "# Merge population data with House election data, matching each election year to the closest available population data year\n",
    "merged_house_data = pd.merge_asof(\n",
    "    house_data,\n",
    "    state_population_aggregated,\n",
    "    on='year',\n",
    "    by='state_po',\n",
    "    direction='nearest'  # Matches the closest available year for each election year\n",
    ")\n",
    "\n",
    "# Save the merged dataset\n",
    "merged_house_data.to_csv(output_house_path, index=False)\n",
    "print(f\"House data with population by age and sex saved to {output_house_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "left keys must be sorted",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m state_population_aggregated \u001b[38;5;241m=\u001b[39m state_population_aggregated\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_po\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Merge population data with House election data, matching each election year to the closest available population data year\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m merged_house_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_asof\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhouse_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_population_aggregated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate_po\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnearest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Matches the closest available year for each election year\u001b[39;49;00m\n\u001b[0;32m     63\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Save the merged dataset\u001b[39;00m\n\u001b[0;32m     66\u001b[0m merged_house_data\u001b[38;5;241m.\u001b[39mto_csv(output_house_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:708\u001b[0m, in \u001b[0;36mmerge_asof\u001b[1;34m(left, right, on, left_on, right_on, left_index, right_index, by, left_by, right_by, suffixes, tolerance, allow_exact_matches, direction)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;124;03mPerform a merge by key distance.\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;124;03m4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    691\u001b[0m op \u001b[38;5;241m=\u001b[39m _AsOfMerge(\n\u001b[0;32m    692\u001b[0m     left,\n\u001b[0;32m    693\u001b[0m     right,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    706\u001b[0m     direction\u001b[38;5;241m=\u001b[39mdirection,\n\u001b[0;32m    707\u001b[0m )\n\u001b[1;32m--> 708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1926\u001b[0m, in \u001b[0;36m_OrderedMerge.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m-> 1926\u001b[0m     join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1928\u001b[0m     left_join_indexer: npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1929\u001b[0m     right_join_indexer: npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1151\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     join_index, right_indexer, left_indexer \u001b[38;5;241m=\u001b[39m _left_join_on_index(\n\u001b[0;32m   1148\u001b[0m         right_ax, left_ax, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1151\u001b[0m     (left_indexer, right_indexer) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_index:\n\u001b[0;32m   1154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:2238\u001b[0m, in \u001b[0;36m_AsOfMerge._get_join_indexers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2235\u001b[0m         tolerance \u001b[38;5;241m=\u001b[39m tolerance\u001b[38;5;241m.\u001b[39m_value\n\u001b[0;32m   2237\u001b[0m \u001b[38;5;66;03m# initial type conversion as needed\u001b[39;00m\n\u001b[1;32m-> 2238\u001b[0m left_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_values_for_libjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2239\u001b[0m right_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_values_for_libjoin(right_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2241\u001b[0m \u001b[38;5;66;03m# a \"by\" parameter requires special handling\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:2182\u001b[0m, in \u001b[0;36m_AsOfMerge._convert_values_for_libjoin\u001b[1;34m(self, values, side)\u001b[0m\n\u001b[0;32m   2180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isna(values)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   2181\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerge keys contain null values on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mside\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m side\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mside\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m keys must be sorted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, ArrowExtensionArray):\n\u001b[0;32m   2185\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39m_maybe_convert_datelike_array()\n",
      "\u001b[1;31mValueError\u001b[0m: left keys must be sorted"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "population_age_sex_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\papildomi _duomenys\\Population by Age and Sex - US, States, Counties.csv\"\n",
    "house_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house.csv\"\n",
    "output_house_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_age_sex.csv\"\n",
    "\n",
    "# Load Population data\n",
    "population_data = pd.read_csv(population_age_sex_data_path)\n",
    "\n",
    "# Filter out U.S.-level data\n",
    "state_county_population_data = population_data[population_data['Description'] != 'U.S.'].copy()\n",
    "\n",
    "# Extract state abbreviations\n",
    "state_county_population_data['state_po'] = state_county_population_data['Description'].apply(\n",
    "    lambda desc: re.search(r', ([A-Z]{2})', desc).group(1) if re.search(r', ([A-Z]{2})', desc) else desc.upper()\n",
    ")\n",
    "\n",
    "# Define population columns to aggregate\n",
    "age_sex_columns = [\n",
    "    'Total Population', 'Population 0-4', 'Population 5-17', 'Population 18-24', \n",
    "    'Population 25-44', 'Population 45-64', 'Population 65+', 'Population Under 18', \n",
    "    'Population 18-54', 'Population 55+', 'Male Population', 'Female Population'\n",
    "]\n",
    "\n",
    "# Convert columns to numeric for proper aggregation\n",
    "state_county_population_data[age_sex_columns] = state_county_population_data[age_sex_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Group by 'Year' and 'state_po' to aggregate county data into state-level totals by summing\n",
    "state_population_aggregated = state_county_population_data.groupby(['Year', 'state_po'], as_index=False)[age_sex_columns].sum()\n",
    "\n",
    "# Rename columns to Lithuanian as required\n",
    "state_population_aggregated = state_population_aggregated.rename(columns={\n",
    "    'Year': 'year',\n",
    "    'Population 0-4': 'Populiacija_0_4',\n",
    "    'Population 5-17': 'Populiacija_5_17',\n",
    "    'Population 18-24': 'Populiacija_18_24',\n",
    "    'Population 25-44': 'Populiacija_25_44',\n",
    "    'Population 45-64': 'Populiacija_45_64',\n",
    "    'Population 65+': 'Populiacija_65_plus',\n",
    "    'Population Under 18': 'Populiacija_under_18',\n",
    "    'Population 18-54': 'Populiacija_18_54',\n",
    "    'Population 55+': 'Populiacija_55_plus',\n",
    "    'Male Population': 'Vyru_populiacija',\n",
    "    'Female Population': 'Moteru_populiacija'\n",
    "})\n",
    "\n",
    "# Load House election data\n",
    "house_data = pd.read_csv(house_data_path)\n",
    "\n",
    "# Sort both datasets by 'state_po' and 'year' to ensure keys are ordered for merge_asof\n",
    "house_data = house_data.sort_values(['state_po', 'year']).reset_index(drop=True)\n",
    "state_population_aggregated = state_population_aggregated.sort_values(['state_po', 'year']).reset_index(drop=True)\n",
    "\n",
    "# Merge population data with House election data, matching each election year to the closest available population data year\n",
    "merged_house_data = pd.merge_asof(\n",
    "    house_data,\n",
    "    state_population_aggregated,\n",
    "    on='year',\n",
    "    by='state_po',\n",
    "    direction='nearest'  # Matches the closest available year for each election year\n",
    ")\n",
    "\n",
    "# Save the merged dataset\n",
    "merged_house_data.to_csv(output_house_path, index=False)\n",
    "print(f\"House data with population by age and sex saved to {output_house_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House data with population by age and sex saved to C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_age_sex.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "population_age_sex_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\papildomi _duomenys\\Population by Age and Sex - US, States, Counties.csv\"\n",
    "house_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house.csv\"\n",
    "output_house_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_age_sex.csv\"\n",
    "\n",
    "# Load Population data\n",
    "population_data = pd.read_csv(population_age_sex_data_path)\n",
    "\n",
    "# Filter out U.S.-level data and focus on state and county data\n",
    "state_county_population_data = population_data[population_data['Description'] != 'U.S.'].copy()\n",
    "\n",
    "# Extract state abbreviations from Description, handling both cases\n",
    "def extract_state_po(description):\n",
    "    match = re.search(r', ([A-Z]{2})$', description)\n",
    "    if match:\n",
    "        return match.group(1)  # Extract state abbreviation if available\n",
    "    else:\n",
    "        return description.strip().upper()  # Use state name directly if no abbreviation is present\n",
    "\n",
    "state_county_population_data['state_po'] = state_county_population_data['Description'].apply(extract_state_po)\n",
    "\n",
    "# Define population columns to sum\n",
    "age_sex_columns = [\n",
    "    'Total Population', 'Population 0-4', 'Population 5-17', 'Population 18-24', \n",
    "    'Population 25-44', 'Population 45-64', 'Population 65+', 'Population Under 18', \n",
    "    'Population 18-54', 'Population 55+', 'Male Population', 'Female Population'\n",
    "]\n",
    "\n",
    "# Convert relevant columns to numeric for aggregation\n",
    "state_county_population_data[age_sex_columns] = state_county_population_data[age_sex_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Group by 'Year' and 'state_po' to aggregate population data at the state-year level\n",
    "state_population_aggregated = state_county_population_data.groupby(['Year', 'state_po'], as_index=False)[age_sex_columns].sum()\n",
    "\n",
    "# Rename columns to Lithuanian as needed\n",
    "state_population_aggregated = state_population_aggregated.rename(columns={\n",
    "    'Year': 'year',\n",
    "    'Population 0-4': 'Populiacija_0_4',\n",
    "    'Population 5-17': 'Populiacija_5_17',\n",
    "    'Population 18-24': 'Populiacija_18_24',\n",
    "    'Population 25-44': 'Populiacija_25_44',\n",
    "    'Population 45-64': 'Populiacija_45_64',\n",
    "    'Population 65+': 'Populiacija_65_plus',\n",
    "    'Population Under 18': 'Populiacija_under_18',\n",
    "    'Population 18-54': 'Populiacija_18_54',\n",
    "    'Population 55+': 'Populiacija_55_plus',\n",
    "    'Male Population': 'Vyru_populiacija',\n",
    "    'Female Population': 'Moteru_populiacija'\n",
    "})\n",
    "\n",
    "# Load House election data\n",
    "house_data = pd.read_csv(house_data_path)\n",
    "\n",
    "# Merge the population data with the House election data using exact year matches\n",
    "merged_house_data = pd.merge(\n",
    "    house_data,\n",
    "    state_population_aggregated,\n",
    "    on=['year', 'state_po'],\n",
    "    how='left'  # Preserve all rows in house_data, filling unmatched rows with NaN\n",
    ")\n",
    "\n",
    "# Save the merged dataset to a new file\n",
    "merged_house_data.to_csv(output_house_path, index=False)\n",
    "print(f\"House data with population by age and sex saved to {output_house_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House duomenys su užpildytomis reikšmėmis išsaugoti C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_age_sex_filled.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Kelias iki sujungto House failo su amžiaus ir lyties duomenimis\n",
    "house_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_age_sex.csv\"\n",
    "output_house_filled_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_age_sex_filled.csv\"\n",
    "\n",
    "# Įkeliame House duomenis\n",
    "house_data = pd.read_csv(house_data_path)\n",
    "\n",
    "# Stulpeliai, kuriuos reikia užpildyti trūkstamomis reikšmėmis\n",
    "columns_to_fill = [\n",
    "    'Populiacija_0_4', 'Populiacija_5_17', 'Populiacija_18_24', 'Populiacija_25_44',\n",
    "    'Populiacija_45_64', 'Populiacija_65_plus', 'Populiacija_under_18', 'Populiacija_18_54',\n",
    "    'Populiacija_55_plus', 'Vyru_populiacija', 'Moteru_populiacija'\n",
    "]\n",
    "\n",
    "# Užpildome trūkstamas reikšmes kiekvienai valstijai atskirai naudodami `ffill()` ir `bfill()`\n",
    "for state, group in house_data.groupby('state_po'):\n",
    "    # Forward fill trūkstamoms reikšmėms iš ankstesnių metų, o jei jų nėra, naudoti vėlesnius\n",
    "    group[columns_to_fill] = group[columns_to_fill].ffill().bfill()\n",
    "    house_data.update(group)\n",
    "\n",
    "# Išsaugome užpildytą failą\n",
    "house_data.to_csv(output_house_filled_path, index=False)\n",
    "print(f\"House duomenys su užpildytomis reikšmėmis išsaugoti {output_house_filled_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Komponentų pokyčių duomenys su naujais stulpelių pavadinimais išsaugoti į C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\Components_of_Population_Change_Renamed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Failo kelias\n",
    "population_change_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\papildomi _duomenys\\Components of Population Change - US, States, Counties.csv\"\n",
    "output_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\Components_of_Population_Change_Renamed.csv\"\n",
    "\n",
    "# Nuskaityti duomenis\n",
    "population_change_data = pd.read_csv(population_change_data_path)\n",
    "\n",
    "# Filtruoti duomenis - išmesti bendrus JAV duomenis, paliekant tik valstijų ir apskričių duomenis\n",
    "state_county_data = population_change_data[population_change_data['Description'] != 'U.S.'].copy()\n",
    "\n",
    "# Išgauti valstijų inicialus iš 'Description' stulpelio\n",
    "def extract_state_po(description):\n",
    "    match = re.search(r', ([A-Z]{2})$', description)\n",
    "    if match:\n",
    "        return match.group(1)  # Jei yra, išgauna valstijos inicialus\n",
    "    else:\n",
    "        return description.strip().upper()  # Jei nėra, naudoja valstijos pavadinimą\n",
    "\n",
    "state_county_data['state_po'] = state_county_data['Description'].apply(extract_state_po)\n",
    "\n",
    "# Stulpeliai, kuriuos norime sumuoti\n",
    "change_columns = ['Births', 'Deaths', 'Net International Migration', 'Net Domestic Migration', 'Residual']\n",
    "\n",
    "# Konvertuoti šiuos stulpelius į skaičius, jei būtų klaidų (ne skaitiniai duomenys)\n",
    "state_county_data[change_columns] = state_county_data[change_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Agreguoti duomenis valstijų lygiu (suminiai duomenys kiekvieniems metams)\n",
    "state_population_change_aggregated = state_county_data.groupby(['Year', 'state_po'], as_index=False)[change_columns].sum()\n",
    "\n",
    "# Pervadinti stulpelius į lietuvių kalbą\n",
    "state_population_change_aggregated = state_population_change_aggregated.rename(columns={\n",
    "    'Year': 'year',\n",
    "    'Births': 'Gimimai',\n",
    "    'Deaths': 'Mirtys',\n",
    "    'Net International Migration': 'Išorinė_migracija',\n",
    "    'Net Domestic Migration': 'Vidinė_migracija',\n",
    "    'Residual': 'Pokytis'\n",
    "})\n",
    "\n",
    "# Išsaugoti rezultatus į naują CSV failą\n",
    "state_population_change_aggregated.to_csv(output_path, index=False)\n",
    "print(f\"Komponentų pokyčių duomenys su naujais stulpelių pavadinimais išsaugoti į {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Komponentų pokyčių duomenys su naujais stulpelių pavadinimais išsaugoti į C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\Components_of_Population_Change_Renamed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Failo kelias\n",
    "population_change_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\papildomi _duomenys\\Components of Population Change - US, States, Counties.csv\"\n",
    "output_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\Components_of_Population_Change_Renamed.csv\"\n",
    "\n",
    "# Nuskaityti duomenis\n",
    "population_change_data = pd.read_csv(population_change_data_path)\n",
    "\n",
    "# Valstijų pavadinimų ir inicialų žodynas\n",
    "state_abbrev = {\n",
    "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA',\n",
    "    'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA',\n",
    "    'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA',\n",
    "    'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO',\n",
    "    'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT',\n",
    "    'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'\n",
    "}\n",
    "\n",
    "# Filtruoti duomenis - išmesti bendrus JAV duomenis, paliekant tik valstijų ir apskričių duomenis\n",
    "state_county_data = population_change_data[population_change_data['Description'] != 'U.S.'].copy()\n",
    "\n",
    "# Funkcija, skirta gauti valstijos inicialus\n",
    "def get_state_po(description):\n",
    "    match = re.search(r', ([A-Z]{2})$', description)  # Patikrina, ar pabaigoje yra inicialai\n",
    "    if match:\n",
    "        return match.group(1)  # Grąžina inicialus, jei jie yra\n",
    "    else:\n",
    "        # Jei nėra inicialų, naudoja žodyną\n",
    "        state_name = description.strip().split(',')[0]\n",
    "        return state_abbrev.get(state_name, state_name)  # Naudoja pavadinimą, jei nėra žodyne\n",
    "\n",
    "# Pritaikyti funkciją\n",
    "state_county_data['state_po'] = state_county_data['Description'].apply(get_state_po)\n",
    "\n",
    "# Stulpeliai, kuriuos norime sumuoti\n",
    "change_columns = ['Births', 'Deaths', 'Net International Migration', 'Net Domestic Migration', 'Residual']\n",
    "\n",
    "# Konvertuoti šiuos stulpelius į skaičius, jei būtų klaidų (ne skaitiniai duomenys)\n",
    "state_county_data[change_columns] = state_county_data[change_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Agreguoti duomenis valstijų lygiu (suminiai duomenys kiekvieniems metams)\n",
    "state_population_change_aggregated = state_county_data.groupby(['Year', 'state_po'], as_index=False)[change_columns].sum()\n",
    "\n",
    "# Pervadinti stulpelius į lietuvių kalbą\n",
    "state_population_change_aggregated = state_population_change_aggregated.rename(columns={\n",
    "    'Year': 'year',\n",
    "    'Births': 'Gimimai',\n",
    "    'Deaths': 'Mirtys',\n",
    "    'Net International Migration': 'Išorinė_migracija',\n",
    "    'Net Domestic Migration': 'Vidinė_migracija',\n",
    "    'Residual': 'Pokytis'\n",
    "})\n",
    "\n",
    "# Išsaugoti rezultatus į naują CSV failą\n",
    "state_population_change_aggregated.to_csv(output_path, index=False)\n",
    "print(f\"Komponentų pokyčių duomenys su naujais stulpelių pavadinimais išsaugoti į {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House duomenys su komponentų pokyčiais išsaugoti į C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_change.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Failų keliai\n",
    "population_change_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\Components_of_Population_Change_Renamed.csv\"\n",
    "house_data_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house.csv\"\n",
    "output_house_path = r\"C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house_with_population_change.csv\"\n",
    "\n",
    "# Užkrauti komponentų pokyčių duomenis\n",
    "population_change_data = pd.read_csv(population_change_data_path)\n",
    "\n",
    "# Užkrauti House rinkimų duomenis\n",
    "house_data = pd.read_csv(house_data_path)\n",
    "\n",
    "# Sujungti komponentų pokyčių duomenis su House duomenimis pagal 'year' ir 'state_po'\n",
    "merged_house_data = pd.merge(\n",
    "    house_data,\n",
    "    population_change_data,\n",
    "    on=['year', 'state_po'],\n",
    "    how='left'  # Paliekame visas eilutes iš house_data, jei nesutampa - užpildys NaN reikšmėmis\n",
    ")\n",
    "\n",
    "# Išsaugoti sujungtą failą\n",
    "merged_house_data.to_csv(output_house_path, index=False)\n",
    "print(f\"House duomenys su komponentų pokyčiais išsaugoti į {output_house_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "incompatible index of inserted column with frame index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:12687\u001b[0m, in \u001b[0;36m_reindex_for_setitem\u001b[1;34m(value, index)\u001b[0m\n\u001b[0;32m  12686\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m> 12687\u001b[0m     reindexed_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m  12688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m  12689\u001b[0m     \u001b[38;5;66;03m# raised in MultiIndex.from_tuples, see test_insert_error_msmgs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:5153\u001b[0m, in \u001b[0;36mSeries.reindex\u001b[1;34m(self, index, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[0;32m   5136\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[0;32m   5137\u001b[0m     NDFrame\u001b[38;5;241m.\u001b[39mreindex,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m   5138\u001b[0m     klass\u001b[38;5;241m=\u001b[39m_shared_doc_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklass\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5151\u001b[0m     tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m-> 5153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:5610\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[1;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[0;32m   5609\u001b[0m \u001b[38;5;66;03m# perform the reindex on the axes\u001b[39;00m\n\u001b[1;32m-> 5610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_axes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5611\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m   5612\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:5633\u001b[0m, in \u001b[0;36mNDFrame._reindex_axes\u001b[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[0;32m   5632\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(a)\n\u001b[1;32m-> 5633\u001b[0m new_index, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\n\u001b[0;32m   5635\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5637\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(a)\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:4433\u001b[0m, in \u001b[0;36mIndex.reindex\u001b[1;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[0;32m   4431\u001b[0m             indexer, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(target)\n\u001b[1;32m-> 4433\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_reindex_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target, indexer\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:2717\u001b[0m, in \u001b[0;36mMultiIndex._wrap_reindex_result\u001b[1;34m(self, target, indexer, preserve_names)\u001b[0m\n\u001b[0;32m   2716\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2717\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[43mMultiIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tuples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   2719\u001b[0m     \u001b[38;5;66;03m# not all tuples, see test_constructor_dict_multiindex_reindex_flat\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:222\u001b[0m, in \u001b[0;36mnames_compat.<locals>.new_meth\u001b[1;34m(self_or_cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_or_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:617\u001b[0m, in \u001b[0;36mMultiIndex.from_tuples\u001b[1;34m(cls, tuples, sortorder, names)\u001b[0m\n\u001b[0;32m    615\u001b[0m         tuples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(tuples\u001b[38;5;241m.\u001b[39m_values)\n\u001b[1;32m--> 617\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuples_to_object_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtuples\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tuples, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32mlib.pyx:3029\u001b[0m, in \u001b[0;36mpandas._libs.lib.tuples_to_object_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Buffer dtype mismatch, expected 'Python object' but got 'long long'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17296\\1536540175.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mmerged_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state_po'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Fill missing values with the nearest available data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcolumns_to_fill\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mmerged_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerged_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'state_po'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mffill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Save the updated file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0moutput_file_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\1976-2020-house-updated-filled.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4307\u001b[0m             \u001b[1;31m# Column to set is duplicated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4308\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4309\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4310\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4311\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4521\u001b[0m         \u001b[0mSeries\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mTimeSeries\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mconformed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mDataFrames\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4522\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4523\u001b[0m         \"\"\"\n\u001b[1;32m-> 4524\u001b[1;33m         \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4526\u001b[0m         if (\n\u001b[0;32m   4527\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5259\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5261\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5262\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5263\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_reindex_for_setitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5266\u001b[0m             \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Paulius\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(value, index)\u001b[0m\n\u001b[0;32m  12690\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12691\u001b[0m             \u001b[1;31m# duplicate axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12692\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 12694\u001b[1;33m         raise TypeError(\n\u001b[0m\u001b[0;32m  12695\u001b[0m             \u001b[1;34m\"incompatible index of inserted column with frame index\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12696\u001b[0m         \u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12697\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mreindexed_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: incompatible index of inserted column with frame index"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the updated House data\n",
    "house_updated_file_path = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\1976-2020-house-updated.csv\"\n",
    "merged_data = pd.read_csv(house_updated_file_path)\n",
    "\n",
    "# Define the columns to fill with nearest data\n",
    "columns_to_fill = [\n",
    "    'Bendras darbingumas',\n",
    "    'Darbas su darbo sutartimi',\n",
    "    'Kasyba',\n",
    "    'Statyba',\n",
    "    'Gamyba',\n",
    "    'Valstijos lygio valdžia',\n",
    "    'Vietinė valdžia'\n",
    "]\n",
    "\n",
    "# Sort the data by state and year for filling missing values\n",
    "merged_data.sort_values(by=['state_po', 'year'], inplace=True)\n",
    "\n",
    "# Fill missing values with the nearest available data\n",
    "for column in columns_to_fill:\n",
    "    merged_data[column] = merged_data.groupby('state_po')[column].apply(lambda group: group.ffill().bfill())\n",
    "\n",
    "# Save the updated file\n",
    "output_file_path = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\1976-2020-house-updated-filled.csv\"\n",
    "merged_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Missing data filled and saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data filled and saved to: C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house-updated-filled.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the updated House data\n",
    "house_updated_file_path = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\1976-2020-house-updated.csv\"\n",
    "merged_data = pd.read_csv(house_updated_file_path)\n",
    "\n",
    "# Define the columns to fill with nearest data\n",
    "columns_to_fill = [\n",
    "    'Bendras darbingumas',\n",
    "    'Darbas su darbo sutartimi',\n",
    "    'Kasyba',\n",
    "    'Statyba',\n",
    "    'Gamyba',\n",
    "    'Valstijos lygio valdžia',\n",
    "    'Vietinė valdžia'\n",
    "]\n",
    "\n",
    "# Sort the data by state and year for filling missing values\n",
    "merged_data.sort_values(by=['state_po', 'year'], inplace=True)\n",
    "\n",
    "# Fill missing values with the nearest available data\n",
    "for column in columns_to_fill:\n",
    "    merged_data[column] = (\n",
    "        merged_data.groupby('state_po', group_keys=False)[column]\n",
    "        .apply(lambda group: group.ffill().bfill())\n",
    "    )\n",
    "\n",
    "# Save the updated file\n",
    "output_file_path = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\1976-2020-house-updated-filled.csv\"\n",
    "merged_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Missing data filled and saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates made: 'population' removed and 'Total Population' renamed. File saved to: C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house-updated.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path for the house data\n",
    "house_file_path = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\1976-2020-house.csv\"\n",
    "\n",
    "# Load the house data\n",
    "house_data = pd.read_csv(house_file_path)\n",
    "\n",
    "# Drop the 'population' column if it exists\n",
    "if 'population' in house_data.columns:\n",
    "    house_data.drop(columns=['population'], inplace=True)\n",
    "\n",
    "# Rename 'Total Population' to 'Populiacija_viso_2'\n",
    "if 'Total Population' in house_data.columns:\n",
    "    house_data.rename(columns={'Total Population': 'Populiacija_viso_2'}, inplace=True)\n",
    "\n",
    "# Save the updated house data\n",
    "updated_house_file_path = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\1976-2020-house-updated.csv\"\n",
    "house_data.to_csv(updated_house_file_path, index=False)\n",
    "\n",
    "print(f\"Updates made: 'population' removed and 'Total Population' renamed. File saved to: {updated_house_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neigiamų reikšmių vietos:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Įkeliame failą\n",
    "failo_kelias = r'C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house.csv'\n",
    "duomenys = pd.read_csv(failo_kelias)\n",
    "\n",
    "# Nurodome stulpelius, kuriuos norime ignoruoti\n",
    "ignoruojami_stulpeliai = ['Pokytis', 'Išorinė_migracija', 'Vidinė_migracija']\n",
    "\n",
    "# Pasirenkame tik skaitinius stulpelius ir pašaliname ignoruojamus\n",
    "skaitiniai_duomenys = duomenys.select_dtypes(include=['number']).drop(columns=ignoruojami_stulpeliai, errors='ignore')\n",
    "\n",
    "# Randame vietas su neigiamomis reikšmėmis\n",
    "neigiamu_vietos = (skaitiniai_duomenys < 0)\n",
    "\n",
    "# Sukuriame rezultatų sąrašą su tiksliomis vietomis\n",
    "neigiamu_reiksmiu_vietos = []\n",
    "for stulpelis in neigiamu_vietos.columns:\n",
    "    eilutes = neigiamu_vietos.index[neigiamu_vietos[stulpelis]].tolist()\n",
    "    if eilutes:\n",
    "        for eilute in eilutes:\n",
    "            neigiamu_reiksmiu_vietos.append((eilute, stulpelis, duomenys.loc[eilute, stulpelis]))\n",
    "\n",
    "# Atvaizduojame rezultatus\n",
    "print(\"Neigiamų reikšmių vietos:\")\n",
    "for vieta in neigiamu_reiksmiu_vietos:\n",
    "    print(f\"Eilutė: {vieta[0]}, Stulpelis: {vieta[1]}, Reikšmė: {vieta[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stulpelis 'mode' pašalintas ir atnaujintas failas išsaugotas: C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house-valyti.csv\n"
     ]
    }
   ],
   "source": [
    "# Pašaliname stulpelį \"mode\"\n",
    "duomenys = duomenys.drop(columns=['mode'], errors='ignore')\n",
    "\n",
    "# Išsaugome atnaujintą failą\n",
    "failo_kelias_naujas = r'C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house-valyti.csv'\n",
    "duomenys.to_csv(failo_kelias_naujas, index=False)\n",
    "\n",
    "print(f\"Stulpelis 'mode' pašalintas ir atnaujintas failas išsaugotas: {failo_kelias_naujas}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stulpeliai 'office' ir 'mode' pašalinti, atnaujintas failas išsaugotas: C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house-updated.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Nurodykite House failo kelią\n",
    "house_file_path = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\1976-2020-house.csv\"\n",
    "\n",
    "# Nuskaitykite House duomenis\n",
    "house_data = pd.read_csv(house_file_path)\n",
    "\n",
    "# Pašalinkite stulpelius 'office' ir 'mode', jei jie egzistuoja\n",
    "columns_to_remove = ['office', 'mode']\n",
    "house_data.drop(columns=[col for col in columns_to_remove if col in house_data.columns], inplace=True)\n",
    "\n",
    "# Išsaugokite atnaujintą failą\n",
    "output_file_path = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\1976-2020-house-updated.csv\"\n",
    "house_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Stulpeliai 'office' ir 'mode' pašalinti, atnaujintas failas išsaugotas: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pašalintas stulpelis 'mode' nes visos jo reiksmes buvo vienodos ir tai neturėtų jokios reikšmės analizei. Tai pat pašalintas ir 'office'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failas atnaujintas ir išsaugotas: C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house-updated.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Nurodome failo kelią\n",
    "failo_kelias = r'C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house.csv'\n",
    "\n",
    "# Įkeliame failą\n",
    "house_duomenys = pd.read_csv(failo_kelias)\n",
    "\n",
    "# Sukuriame party_simplified stulpelį\n",
    "def supaprastinta_partija(partija):\n",
    "    if isinstance(partija, str):  # Patikriname, ar reikšmė yra tekstas\n",
    "        if 'DEMOCRAT' in partija.upper():\n",
    "            return 'DEMOCRAT'\n",
    "        elif 'REPUBLICAN' in partija.upper():\n",
    "            return 'REPUBLICAN'\n",
    "        elif 'LIBERTARIAN' in partija.upper():\n",
    "            return 'LIBERTARIAN'\n",
    "    return 'OTHER'  # Jei nėra teksto arba neatitinka kriterijų\n",
    "\n",
    "house_duomenys['party_simplified'] = house_duomenys['party'].apply(supaprastinta_partija)\n",
    "\n",
    "# Išsaugome failą su naujais duomenimis\n",
    "naujas_kelias = r'C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house-updated.csv'\n",
    "house_duomenys.to_csv(naujas_kelias, index=False)\n",
    "\n",
    "print(f\"Failas atnaujintas ir išsaugotas: {naujas_kelias}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atnaujintas failas išsaugotas: C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house-updated.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Nurodykite failo kelią\n",
    "house_file_path = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\1976-2020-house.csv\"\n",
    "\n",
    "# Nuskaitykite „House“ duomenis\n",
    "house_data = pd.read_csv(house_file_path)\n",
    "\n",
    "# Grupavimas pagal metus ir valstiją, surandant daugiausiai balsų gavusį kandidatą\n",
    "house_data['laimetojas'] = house_data.groupby(['year', 'state'])['candidatevotes'].transform(\n",
    "    lambda x: x == x.max()\n",
    ")\n",
    "\n",
    "# Paversti „laimetojas“ stulpelį į bool tipo reikšmes\n",
    "house_data['laimetojas'] = house_data['laimetojas'].astype(bool)\n",
    "\n",
    "# Išsaugokite atnaujintą failą\n",
    "output_file_path = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\1976-2020-house-updated.csv\"\n",
    "house_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Atnaujintas failas išsaugotas: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atnaujintas failas išsaugotas: C:\\Users\\Paulius\\Duomenu mokslas\\projektas_US_rinkimai\\1976-2020-house-updated.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Nurodykite failo kelią\n",
    "house_file_path = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\1976-2020-house.csv\"\n",
    "\n",
    "# Nuskaitykite „House“ duomenis\n",
    "house_data = pd.read_csv(house_file_path)\n",
    "\n",
    "# Grupavimas pagal metus, valstijas ir apygardas, surandant daugiausiai balsų gavusį kandidatą\n",
    "house_data['laimetojas'] = house_data.groupby(['year', 'state', 'district'])['candidatevotes'].transform(\n",
    "    lambda x: x == x.max()\n",
    ")\n",
    "\n",
    "# Paversti „laimetojas“ stulpelį į bool tipo reikšmes\n",
    "house_data['laimetojas'] = house_data['laimetojas'].astype(bool)\n",
    "\n",
    "# Išsaugokite atnaujintą failą\n",
    "output_file_path = \"C:\\\\Users\\\\Paulius\\\\Duomenu mokslas\\\\projektas_US_rinkimai\\\\1976-2020-house-updated.csv\"\n",
    "house_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Atnaujintas failas išsaugotas: {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
